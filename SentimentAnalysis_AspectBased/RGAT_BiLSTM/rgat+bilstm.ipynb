{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from datasets import my_collate, my_collate_elmo, load_datasets_and_vocabs\n",
    "from tree import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "class Highway(nn.Module):\n",
    "    def __init__(self, layer_num, dim):\n",
    "        super().__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.linear = nn.ModuleList([nn.Linear(dim, dim)\n",
    "                                     for _ in range(layer_num)])\n",
    "        self.gate = nn.ModuleList([nn.Linear(dim, dim)\n",
    "                                   for _ in range(layer_num)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.layer_num):\n",
    "            gate = F.sigmoid(self.gate[i](x))\n",
    "            nonlinear = F.relu(self.linear[i](x))\n",
    "            x = gate * nonlinear + (1 - gate) * x\n",
    "        return x\n",
    "    \n",
    "def mask_logits(target, mask):\n",
    "    return target * mask + (1 - mask) * (-1e30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationAttention(nn.Module):\n",
    "    def __init__(self, in_dim = 300, hidden_dim = 64):\n",
    "        # in_dim: the dimension fo query vector\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, feature, dep_tags_v, dmask):\n",
    "        '''\n",
    "        C feature/context [N, L, D]\n",
    "        Q dep_tags_v          [N, L, D]\n",
    "        mask dmask          [N, L]\n",
    "        '''\n",
    "        Q = self.fc1(dep_tags_v)\n",
    "        Q = self.relu(Q)\n",
    "        Q = self.fc2(Q)  # (N, L, 1)\n",
    "        Q = Q.squeeze(2)\n",
    "        Q = F.softmax(mask_logits(Q, dmask), dim=1)\n",
    "\n",
    "        Q = Q.unsqueeze(2)\n",
    "        out = torch.bmm(feature.transpose(1, 2), Q)\n",
    "        out = out.squeeze(2)\n",
    "        # out = F.sigmoid(out)\n",
    "        return out  # ([N, L])\n",
    "    \n",
    "    \n",
    "class DotprodAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, feature, aspect_v, dmask):\n",
    "        '''\n",
    "        C feature/context [N, L, D]\n",
    "        Q dep_tags_v          [N, D]\n",
    "        mask dmask          [N, L]\n",
    "        '''\n",
    "\n",
    "        Q = aspect_v\n",
    "        Q = Q.unsqueeze(2)  # (N, D, 1)\n",
    "        dot_prod = torch.bmm(feature, Q)  # (N, L, 1)\n",
    "        dmask = dmask.unsqueeze(2)  # (N, D, 1)\n",
    "        attention_weight = mask_logits(dot_prod, dmask)  # (N, L ,1)\n",
    "        attention = F.softmax(attention_weight, dim=1)  # (N, L, 1)\n",
    "\n",
    "        out = torch.bmm(feature.transpose(1, 2), attention)  # (N, D, 1)\n",
    "        out = out.squeeze(2)\n",
    "        # out = F.sigmoid(out)\n",
    "        # (N, D), ([N, L]), (N, L, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT_Aspect(nn.Module):\n",
    "    \"\"\"\n",
    "    Full model in reshaped tree\n",
    "    \"\"\"\n",
    "    def __init__(self, args, dep_tag_num, pos_tag_num):\n",
    "        super(GAT_Aspect, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        num_embeddings, embed_dim = args.glove_embedding.shape\n",
    "        self.embed = nn.Embedding(num_embeddings, embed_dim)\n",
    "        self.embed.weight = nn.Parameter(\n",
    "            args.glove_embedding, requires_grad=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        if args.highway:\n",
    "            print(\"Inside highway\")\n",
    "            self.highway_dep = Highway(args.num_layers, args.embedding_dim)\n",
    "            self.highway = Highway(args.num_layers, args.embedding_dim)\n",
    "\n",
    "        self.bilstm = nn.LSTM(input_size=args.embedding_dim, hidden_size=args.hidden_size,\n",
    "                              bidirectional=True, batch_first=True, num_layers=args.num_layers)\n",
    "        gcn_input_dim = args.hidden_size * 2\n",
    "\n",
    "        self.gat_dep = [RelationAttention(in_dim = args.embedding_dim).to(args.device) for i in range(args.num_heads)]\n",
    "        self.gat = [DotprodAttention().to(args.device) for i in range(args.num_heads)]\n",
    "\n",
    "        self.dep_embed = nn.Embedding(dep_tag_num, args.embedding_dim)\n",
    "\n",
    "        last_hidden_size = args.hidden_size * 4\n",
    "\n",
    "        layers = [\n",
    "            nn.Linear(last_hidden_size, args.final_hidden_size), nn.ReLU()]\n",
    "        for _ in range(args.num_mlps-1):\n",
    "            layers += [nn.Linear(args.final_hidden_size,\n",
    "                                 args.final_hidden_size), nn.ReLU()]\n",
    "        self.fcs = nn.Sequential(*layers)\n",
    "        self.fc_final = nn.Linear(args.final_hidden_size, args.num_classes)\n",
    "\n",
    "    def forward(self, sentence, aspect, pos_class, dep_tags, text_len, aspect_len, dep_rels, dep_heads, aspect_position, dep_dirs):\n",
    "        '''\n",
    "        Forward takes:\n",
    "            sentence: sentence_id of size (batch_size, text_length)\n",
    "            aspect: aspect_id of size (batch_size, aspect_length)\n",
    "            pos_class: pos_tag_id of size (batch_size, text_length)\n",
    "            dep_tags: dep_tag_id of size (batch_size, text_length)\n",
    "            text_len: (batch_size,) length of each sentence\n",
    "            aspect_len: (batch_size, ) aspect length of each sentence\n",
    "            dep_rels: (batch_size, text_length) relation\n",
    "            dep_heads: (batch_size, text_length) which node adjacent to that node\n",
    "            aspect_position: (batch_size, text_length) mask, with the position of aspect as 1 and others as 0\n",
    "            dep_dirs: (batch_size, text_length) the directions each node to the aspect\n",
    "        '''\n",
    "        fmask = (torch.zeros_like(sentence) != sentence).float()  # (Nï¼ŒL)\n",
    "        dmask = (torch.zeros_like(dep_tags) != dep_tags).float()  # (N ,L)\n",
    "\n",
    "        feature = self.embed(sentence)  # (N, L, D)\n",
    "        aspect_feature = self.embed(aspect) # (N, L', D)\n",
    "        feature = self.dropout(feature)\n",
    "        aspect_feature = self.dropout(aspect_feature)\n",
    "\n",
    "        if self.args.highway:\n",
    "            feature = self.highway(feature)\n",
    "            aspect_feature = self.highway(aspect_feature)\n",
    "\n",
    "        feature, _ = self.bilstm(feature) # (N,L,D)\n",
    "        aspect_feature, _ = self.bilstm(aspect_feature) #(N,L,D)\n",
    "\n",
    "        aspect_feature = aspect_feature.mean(dim = 1) # (N, D)\n",
    "\n",
    "        ############################################################################################\n",
    "        # do gat thing\n",
    "        dep_feature = self.dep_embed(dep_tags)\n",
    "        if self.args.highway:\n",
    "            dep_feature = self.highway_dep(dep_feature)\n",
    "\n",
    "        dep_out = [g(feature, dep_feature, fmask).unsqueeze(1) for g in self.gat_dep]  # (N, 1, D) * num_heads\n",
    "        dep_out = torch.cat(dep_out, dim = 1) # (N, H, D)\n",
    "        dep_out = dep_out.mean(dim = 1) # (N, D)\n",
    "\n",
    "        gat_out = [g(feature, aspect_feature, fmask).unsqueeze(1) for g in self.gat]\n",
    "        gat_out = torch.cat(gat_out, dim=1)\n",
    "        gat_out = gat_out.mean(dim=1)\n",
    "\n",
    "        feature_out = torch.cat([dep_out,  gat_out], dim = 1) # (N, D')\n",
    "        # feature_out = gat_out\n",
    "        #############################################################################################\n",
    "        x = self.dropout(feature_out)\n",
    "        x = self.fcs(x)\n",
    "        logit = self.fc_final(x)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "def get_input_from_batch(args, batch):\n",
    "    # sentence_ids, aspect_ids, dep_tag_ids, pos_class, text_len, aspect_len, sentiment, dep_rel_ids, dep_heads, aspect_positions\n",
    "    inputs = {  'sentence': batch[0],\n",
    "                'aspect': batch[1], # aspect token\n",
    "                'dep_tags': batch[2], # reshaped\n",
    "                'pos_class': batch[3],\n",
    "                'text_len': batch[4],\n",
    "                'aspect_len': batch[5],\n",
    "                'dep_rels': batch[7], # adj no-reshape\n",
    "                'dep_heads': batch[8],\n",
    "                'aspect_position': batch[9],\n",
    "                'dep_dirs': batch[10]\n",
    "                }\n",
    "    labels = batch[6]\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def get_collate_fn(args):\n",
    "    embedding_type = args.embedding_type\n",
    "    if embedding_type == 'glove':\n",
    "        return my_collate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, test_dataset):\n",
    "    '''Train the model'''\n",
    "    tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    collate_fn = get_collate_fn(args)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler,\n",
    "                                  batch_size=args.train_batch_size,\n",
    "                                  collate_fn=collate_fn)\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (\n",
    "            len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(\n",
    "            train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    \n",
    "    \n",
    "    parameters = filter(lambda param: param.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=args.learning_rate)\n",
    "\n",
    "    # Train\n",
    "    print(\"***** Running training *****\")\n",
    "    print(\"Num examples: {}; Epochs: {}; Gradient Accumulation steps: {}; Total optimization steps: {}\".format(len(train_dataset), args.num_train_epochs, args.gradient_accumulation_steps, t_total))\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    all_eval_results = []\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "    set_seed(args)\n",
    "    for _ in train_iterator:\n",
    "        # epoch_iterator = tqdm(train_dataloader, desc='Iteration')\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "            inputs, labels = get_input_from_batch(args, batch)\n",
    "            #print(inputs['sentence'])\n",
    "            \n",
    "            #print(inputs.shape, labels.shape, batch.shape)\n",
    "            logit = model(**inputs)\n",
    "            loss = F.cross_entropy(logit, labels)\n",
    "\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                # scheduler.step()  # Update learning rate schedule\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                # Log metrics\n",
    "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    results, eval_loss = evaluate(args, test_dataset, model)\n",
    "                    all_eval_results.append(results)\n",
    "                    for key, value in results.items():\n",
    "                        tb_writer.add_scalar(\n",
    "                            'eval_{}'.format(key), value, global_step)\n",
    "                    tb_writer.add_scalar('eval_loss', eval_loss, global_step)\n",
    "                    # tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\n",
    "                        'train_loss', (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                # Save model checkpoint\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            epoch_iterator.close()\n",
    "            break\n",
    "\n",
    "    tb_writer.close()\n",
    "    return global_step, tr_loss/global_step, all_eval_results\n",
    "\n",
    "\n",
    "def evaluate(args, eval_dataset, model):\n",
    "    results = {}\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    collate_fn = get_collate_fn(args)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,\n",
    "                                 batch_size=args.eval_batch_size,\n",
    "                                 collate_fn=collate_fn)\n",
    "\n",
    "    # Eval\n",
    "    print(\"***** Running evaluation *****\")\n",
    "    print(\"Num examples: {}; Batch size: {}\".format(len(eval_dataset), args.eval_batch_size))\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for batch in eval_dataloader:\n",
    "    # for batch in tqdm(eval_dataloader, desc='Evaluating'):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = get_input_from_batch(args, batch)\n",
    "\n",
    "            logits = model(**inputs)\n",
    "            tmp_eval_loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = labels.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(\n",
    "                out_label_ids, labels.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    # print(preds)\n",
    "    result = compute_metrics(preds, out_label_ids)\n",
    "    results.update(result)\n",
    "\n",
    "    output_eval_file = os.path.join(args.output_dir, 'eval_results.txt')\n",
    "    with open(output_eval_file, 'a+') as writer:\n",
    "        print('***** Eval results *****')\n",
    "        print(\"eval loss: {}\".format(eval_loss))\n",
    "        for key in sorted(result.keys()):\n",
    "            print(\"{} = {}\".format(key, str(result[key])))\n",
    "    return results, eval_loss\n",
    "\n",
    "\n",
    "def evaluate_badcase(args, eval_dataset, model, word_vocab):\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    collate_fn = get_collate_fn(args)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,\n",
    "                                 batch_size=1,\n",
    "                                 collate_fn=collate_fn)\n",
    "\n",
    "    # Eval\n",
    "    badcases = []\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for batch in eval_dataloader:\n",
    "    # for batch in tqdm(eval_dataloader, desc='Evaluating'):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = get_input_from_batch(args, batch)\n",
    "\n",
    "            logits = model(**inputs)\n",
    "\n",
    "        pred = int(np.argmax(logits.detach().cpu().numpy(), axis=1)[0])\n",
    "        label = int(labels.detach().cpu().numpy()[0])\n",
    "        if pred != label:\n",
    "            sent_ids = inputs['sentence'][0].detach().cpu().numpy()\n",
    "            aspect_ids = inputs['aspect'][0].detach().cpu().numpy()\n",
    "            case = {}\n",
    "            case['sentence'] = ' '.join([word_vocab['itos'][i] for i in sent_ids])\n",
    "            case['aspect'] = ' '.join([word_vocab['itos'][i] for i in aspect_ids])\n",
    "            case['pred'] = pred\n",
    "            case['label'] = label\n",
    "            badcases.append(case)\n",
    "\n",
    "    return badcases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    return acc_and_f1(preds, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = Parameters()\n",
    "# Required parameters\n",
    "parameters.dataset_name = \"rest\"\n",
    "parameters.output_dir = '/data/output-gcn'\n",
    "parameters.num_classes = 3\n",
    "parameters.seed = 2019\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "parameters.glove_dir = \"glove\"\n",
    "parameters.num_layers = 2\n",
    "parameters.add_non_connect = True\n",
    "parameters.multi_hop = True\n",
    "parameters.max_hop = 4\n",
    "parameters.num_heads = 7\n",
    "parameters.dropout = 0.8\n",
    "parameters.num_gcn_layers = 1\n",
    "parameters.gcn_mem_dim = 300\n",
    "parameters.gcn_dropout = 0.2\n",
    "parameters.highway = 'store_true'\n",
    "\n",
    "# GAT\n",
    "parameters.gat_attention_type = 'dotprod'\n",
    "parameters.embedding_type = 'glove'\n",
    "parameters.embedding_dim = 300\n",
    "parameters.dep_relation_embed_dim = 300    \n",
    "parameters.hidden_size = 300 \n",
    "parameters.final_hidden_size = 300 \n",
    "parameters.num_mlps = 2\n",
    "\n",
    "# Training parameters\n",
    "parameters.per_gpu_train_batch_size = 16\n",
    "parameters.per_gpu_eval_batch_size = 32\n",
    "parameters.gradient_accumulation_steps = 2\n",
    "parameters.learning_rate = 1e-3\n",
    "parameters.weight_decay = 0.0\n",
    "parameters.adam_epsilon = 1e-8\n",
    "parameters.max_grad_norm = 1.0\n",
    "parameters.num_train_epochs = 30.0\n",
    "parameters.max_steps = -1\n",
    "parameters.logging_steps = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n",
      "# Read %s Train set: %d rest 1978\n",
      "# Read %s Test set: %d rest 600\n",
      "*** Start processing data(unrolling and reshaping) ***\n",
      "Total sentiment counter: %s defaultdict(<class 'int'>, {'negative': 805, 'positive': 2164, 'neutral': 633})\n",
      "Multi-Aspect-Multi-Sentiment counter: %s defaultdict(<class 'int'>, {'positive': 351, 'neutral': 288, 'negative': 301})\n",
      "*** Start processing data(unrolling and reshaping) ***\n",
      "Total sentiment counter: %s defaultdict(<class 'int'>, {'positive': 728, 'neutral': 196, 'negative': 196})\n",
      "Multi-Aspect-Multi-Sentiment counter: %s defaultdict(<class 'int'>, {'positive': 85, 'neutral': 83, 'negative': 60})\n",
      "****** After unrolling ******\n",
      "Train set size: %s 3602\n",
      "Test set size: %s, 1120\n",
      "{'itos': ['<pad>', '<unk>', 'non-connect', 'ncon_3', 'ncon_2', 'ncon_4', 'det', 'nsubj', 'amod', 'conj', 'pobj', 'dobj', 'cc', 'prep', 'dep', 'poss', 'nn', 'advmod', 'rcmod', 'appos', 'cop', 'nsubjpass', 'num', 'partmod', 'aux', 'ccomp', 'advcl', 'predet', 'auxpass', 'infmod', 'xcomp', 'neg', 'mark', 'possessive', 'pcomp', 'iobj', 'parataxis', 'csubj', 'npadvmod', 'tmod', 'preconj', 'prt', 'expl', 'acomp', 'discourse', 'mwe'], 'stoi': defaultdict(<function _default_unk_index at 0x000001516C8DD790>, {'<pad>': 0, '<unk>': 1, 'non-connect': 2, 'ncon_3': 3, 'ncon_2': 4, 'ncon_4': 5, 'det': 6, 'nsubj': 7, 'amod': 8, 'conj': 9, 'pobj': 10, 'dobj': 11, 'cc': 12, 'prep': 13, 'dep': 14, 'poss': 15, 'nn': 16, 'advmod': 17, 'rcmod': 18, 'appos': 19, 'cop': 20, 'nsubjpass': 21, 'num': 22, 'partmod': 23, 'aux': 24, 'ccomp': 25, 'advcl': 26, 'predet': 27, 'auxpass': 28, 'infmod': 29, 'xcomp': 30, 'neg': 31, 'mark': 32, 'possessive': 33, 'pcomp': 34, 'iobj': 35, 'parataxis': 36, 'csubj': 37, 'npadvmod': 38, 'tmod': 39, 'preconj': 40, 'prt': 41, 'expl': 42, 'acomp': 43, 'discourse': 44, 'mwe': 45, 'root': 1, 'punct': 1, 'number': 1, 'csubjpass': 1, 'quantmod': 1}), 'len': 46}\n",
      "Inside highway\n",
      "***** Running training *****\n",
      "Num examples: 3602; Epochs: 30.0; Gradient Accumulation steps: 2; Total optimization steps: 3390.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                                                                    | 0/30 [00:00<?, ?it/s]C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "# Setup CUDA, GPU training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "parameters.device = device\n",
    "print('Device is {}'.format(parameters.device))\n",
    "\n",
    "# Load datasets and vocabs\n",
    "train_dataset, test_dataset, word_vocab, dep_tag_vocab, pos_tag_vocab= load_datasets_and_vocabs(parameters)\n",
    "\n",
    "print(dep_tag_vocab)\n",
    "\n",
    "# Build Model\n",
    "# model = Aspect_Text_Multi_Syntax_Encoding(args, dep_tag_vocab['len'], pos_tag_vocab['len'])\n",
    "\n",
    "model = GAT_Aspect(parameters, dep_tag_vocab['len'], pos_tag_vocab['len']) # R-GAT with reshaped tree\n",
    "\n",
    "model.to(parameters.device)\n",
    "# Train\n",
    "_, _,  all_eval_results = train(parameters, train_dataset, model, test_dataset)\n",
    "\n",
    "if len(all_eval_results):\n",
    "    best_eval_result = max(all_eval_results, key=lambda x: x['acc']) \n",
    "    for key in sorted(best_eval_result.keys()):\n",
    "        print(\"  %s = %s\", key, str(best_eval_result[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "a04f5d07b0747026a8fbcdf50b9443318e69b1b8bd6247d88bfadb4789282972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
