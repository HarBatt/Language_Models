{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from model_utils import mask_logits\n",
    "from tree import *\n",
    "from datasets import load_datasets_and_vocabs\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AdamW\n",
    "from datasets import my_collate, my_collate_elmo, my_collate_pure_bert, my_collate_bert\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertConfig, BertPreTrainedModel, BertTokenizer\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from model_utils import LinearAttention, DotprodAttention, RelationAttention, Highway, mask_logits\n",
    "from tree import *\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import argparse\n",
    "import codecs\n",
    "import json\n",
    "import linecache\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import simplejson as json\n",
    "import torch\n",
    "from allennlp.modules.elmo import batch_to_ids\n",
    "from lxml import etree\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_datasets_and_vocabs(args):\n",
    "    train, test = get_dataset(args.dataset_name)\n",
    "\n",
    "    # Our model takes unrolled data, currently we don't consider the MAMS cases(future experiments)\n",
    "    _, train_all_unrolled, _, _ = get_rolled_and_unrolled_data(train, args)\n",
    "    _, test_all_unrolled, _, _ = get_rolled_and_unrolled_data(test, args)\n",
    "\n",
    "    logger.info('****** After unrolling ******')\n",
    "    logger.info('Train set size: %s', len(train_all_unrolled))\n",
    "    logger.info('Test set size: %s,', len(test_all_unrolled))\n",
    "\n",
    "    # Build word vocabulary(part of speech, dep_tag) and save pickles.\n",
    "    word_vecs, word_vocab, dep_tag_vocab, pos_tag_vocab = load_and_cache_vocabs(\n",
    "        train_all_unrolled+test_all_unrolled, args)\n",
    "\n",
    "    train_dataset = ASBA_Depparsed_Dataset(\n",
    "        train_all_unrolled, args, word_vocab, dep_tag_vocab, pos_tag_vocab)\n",
    "    test_dataset = ASBA_Depparsed_Dataset(\n",
    "        test_all_unrolled, args, word_vocab, dep_tag_vocab, pos_tag_vocab)\n",
    "\n",
    "    return train_dataset, test_dataset, word_vocab, dep_tag_vocab, pos_tag_vocab\n",
    "\n",
    "\n",
    "def read_sentence_depparsed(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        return data\n",
    "\n",
    "\n",
    "def get_dataset(dataset_name):\n",
    "    '''\n",
    "    Already preprocess the data and now they are in json format.(only for semeval14)\n",
    "    Retrieve train and test set\n",
    "    With a list of dict:\n",
    "    e.g. {\"sentence\": \"Boot time is super fast, around anywhere from 35 seconds to 1 minute.\",\n",
    "    \"tokens\": [\"Boot\", \"time\", \"is\", \"super\", \"fast\", \",\", \"around\", \"anywhere\", \"from\", \"35\", \"seconds\", \"to\", \"1\", \"minute\", \".\"],\n",
    "    \"tags\": [\"NNP\", \"NN\", \"VBZ\", \"RB\", \"RB\", \",\", \"RB\", \"RB\", \"IN\", \"CD\", \"NNS\", \"IN\", \"CD\", \"NN\", \".\"],\n",
    "    \"predicted_dependencies\": [\"nn\", \"nsubj\", \"root\", \"advmod\", \"advmod\", \"punct\", \"advmod\", \"advmod\", \"prep\", \"num\", \"pobj\", \"prep\", \"num\", \"pobj\", \"punct\"],\n",
    "    \"predicted_heads\": [2, 3, 0, 5, 3, 5, 8, 5, 8, 11, 9, 9, 14, 12, 3],\n",
    "    \"dependencies\": [[\"nn\", 2, 1], [\"nsubj\", 3, 2], [\"root\", 0, 3], [\"advmod\", 5, 4], [\"advmod\", 3, 5], [\"punct\", 5, 6], [\"advmod\", 8, 7], [\"advmod\", 5, 8],\n",
    "                    [\"prep\", 8, 9], [\"num\", 11, 10], [\"pobj\", 9, 11], [\"prep\", 9, 12], [\"num\", 14, 13], [\"pobj\", 12, 14], [\"punct\", 3, 15]],\n",
    "    \"aspect_sentiment\": [[\"Boot time\", \"positive\"]], \"from_to\": [[0, 2]]}\n",
    "    '''\n",
    "    rest_train = 'data/semeval14/Restaurants_Train_v2_biaffine_depparsed_with_energy.json'\n",
    "    rest_test = 'data/semeval14/Restaurants_Test_Gold_biaffine_depparsed_with_energy.json'\n",
    "\n",
    "    laptop_train = 'data/semeval14/Laptop_Train_v2_biaffine_depparsed.json'\n",
    "    laptop_test = 'data/semeval14/Laptops_Test_Gold_biaffine_depparsed.json'\n",
    "\n",
    "    twitter_train = 'data/twitter/train_biaffine.json'\n",
    "    twitter_test = 'data/twitter/test_biaffine.json'\n",
    "\n",
    "    ds_train = {'rest': rest_train,\n",
    "                'laptop': laptop_train, 'twitter': twitter_train}\n",
    "    ds_test = {'rest': rest_test,\n",
    "               'laptop': laptop_test, 'twitter': twitter_test}\n",
    "\n",
    "    train = list(read_sentence_depparsed(ds_train[dataset_name]))\n",
    "    logger.info('# Read %s Train set: %d', dataset_name, len(train))\n",
    "\n",
    "    test = list(read_sentence_depparsed(ds_test[dataset_name]))\n",
    "    logger.info(\"# Read %s Test set: %d\", dataset_name, len(test))\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def reshape_dependency_tree_new(as_start, as_end, dependencies, multi_hop=False, add_non_connect=False, tokens=None, max_hop = 5):\n",
    "    '''\n",
    "    Adding multi hops\n",
    "    This function is at the core of our algo, it reshape the dependency tree and center on the aspect.\n",
    "    In open-sourced edition, I choose not to take energy(the soft prediction of dependency from parser)\n",
    "    into consideration. For it requires tweaking allennlp's source code, and the energy is space-consuming.\n",
    "    And there are no significant difference in performance between the soft and the hard(with non-connect) version.\n",
    "\n",
    "    '''\n",
    "    dep_tag = []\n",
    "    dep_idx = []\n",
    "    dep_dir = []\n",
    "    # 1 hop\n",
    "\n",
    "    for i in range(as_start, as_end):\n",
    "        for dep in dependencies:\n",
    "            if i == dep[1] - 1:\n",
    "                # not root, not aspect\n",
    "                if (dep[2] - 1 < as_start or dep[2] - 1 >= as_end) and dep[2] != 0 and dep[2] - 1 not in dep_idx:\n",
    "                    if str(dep[0]) != 'punct':  # and tokens[dep[2] - 1] not in stopWords\n",
    "                        dep_tag.append(dep[0])\n",
    "                        dep_dir.append(1)\n",
    "                    else:\n",
    "                        dep_tag.append('<pad>')\n",
    "                        dep_dir.append(0)\n",
    "                    dep_idx.append(dep[2] - 1)\n",
    "            elif i == dep[2] - 1:\n",
    "                # not root, not aspect\n",
    "                if (dep[1] - 1 < as_start or dep[1] - 1 >= as_end) and dep[1] != 0 and dep[1] - 1 not in dep_idx:\n",
    "                    if str(dep[0]) != 'punct':  # and tokens[dep[1] - 1] not in stopWords\n",
    "                        dep_tag.append(dep[0])\n",
    "                        dep_dir.append(2)\n",
    "                    else:\n",
    "                        dep_tag.append('<pad>')\n",
    "                        dep_dir.append(0)\n",
    "                    dep_idx.append(dep[1] - 1)\n",
    "\n",
    "    if multi_hop:\n",
    "        current_hop = 2\n",
    "        added = True\n",
    "        while current_hop <= max_hop and len(dep_idx) < len(tokens) and added:\n",
    "            added = False\n",
    "            dep_idx_temp = deepcopy(dep_idx)\n",
    "            for i in dep_idx_temp:\n",
    "                for dep in dependencies:\n",
    "                    if i == dep[1] - 1:\n",
    "                        # not root, not aspect\n",
    "                        if (dep[2] - 1 < as_start or dep[2] - 1 >= as_end) and dep[2] != 0 and dep[2] - 1 not in dep_idx:\n",
    "                            if str(dep[0]) != 'punct':  # and tokens[dep[2] - 1] not in stopWords\n",
    "                                dep_tag.append('ncon_'+str(current_hop))\n",
    "                                dep_dir.append(1)\n",
    "                            else:\n",
    "                                dep_tag.append('<pad>')\n",
    "                                dep_dir.append(0)\n",
    "                            dep_idx.append(dep[2] - 1)\n",
    "                            added = True\n",
    "                    elif i == dep[2] - 1:\n",
    "                        # not root, not aspect\n",
    "                        if (dep[1] - 1 < as_start or dep[1] - 1 >= as_end) and dep[1] != 0 and dep[1] - 1 not in dep_idx:\n",
    "                            if str(dep[0]) != 'punct':  # and tokens[dep[1] - 1] not in stopWords\n",
    "                                dep_tag.append('ncon_'+str(current_hop))\n",
    "                                dep_dir.append(2)\n",
    "                            else:\n",
    "                                dep_tag.append('<pad>')\n",
    "                                dep_dir.append(0)\n",
    "                            dep_idx.append(dep[1] - 1)\n",
    "                            added = True\n",
    "            current_hop += 1\n",
    "\n",
    "    if add_non_connect:\n",
    "        for idx, token in enumerate(tokens):\n",
    "            if idx not in dep_idx and (idx < as_start or idx >= as_end):\n",
    "                dep_tag.append('non-connect')\n",
    "                dep_dir.append(0)\n",
    "                dep_idx.append(idx)\n",
    "\n",
    "    # add aspect and index, to make sure length matches len(tokens)\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if idx not in dep_idx:\n",
    "            dep_tag.append('<pad>')\n",
    "            dep_dir.append(0)\n",
    "            dep_idx.append(idx)\n",
    "\n",
    "    index = [i[0] for i in sorted(enumerate(dep_idx), key=lambda x:x[1])]\n",
    "    dep_tag = [dep_tag[i] for i in index]\n",
    "    dep_idx = [dep_idx[i] for i in index]\n",
    "    dep_dir = [dep_dir[i] for i in index]\n",
    "\n",
    "    assert len(tokens) == len(dep_idx), 'length wrong'\n",
    "    return dep_tag, dep_idx, dep_dir\n",
    "\n",
    "\n",
    "def reshape_dependency_tree(as_start, as_end, dependencies, add_2hop=False, add_non_connect=False, tokens=None):\n",
    "    '''\n",
    "    This function is at the core of our algo, it reshape the dependency tree and center on the aspect.\n",
    "\n",
    "    In open-sourced edition, I choose not to take energy(the soft prediction of dependency from parser)\n",
    "    into consideration. For it requires tweaking allennlp's source code, and the energy is space-consuming.\n",
    "    And there are no significant difference in performance between the soft and the hard(with non-connect) version.\n",
    "\n",
    "    '''\n",
    "    dep_tag = []\n",
    "    dep_idx = []\n",
    "    dep_dir = []\n",
    "    # 1 hop\n",
    "\n",
    "    for i in range(as_start, as_end):\n",
    "        for dep in dependencies:\n",
    "            if i == dep[1] - 1:\n",
    "                # not root, not aspect\n",
    "                if (dep[2] - 1 < as_start or dep[2] - 1 >= as_end) and dep[2] != 0 and dep[2] - 1 not in dep_idx:\n",
    "                    if str(dep[0]) != 'punct':  # and tokens[dep[2] - 1] not in stopWords\n",
    "                        dep_tag.append(dep[0])\n",
    "                        dep_dir.append(1)\n",
    "                    else:\n",
    "                        dep_tag.append('<pad>')\n",
    "                        dep_dir.append(0)\n",
    "                    dep_idx.append(dep[2] - 1)\n",
    "            elif i == dep[2] - 1:\n",
    "                # not root, not aspect\n",
    "                if (dep[1] - 1 < as_start or dep[1] - 1 >= as_end) and dep[1] != 0 and dep[1] - 1 not in dep_idx:\n",
    "                    if str(dep[0]) != 'punct':  # and tokens[dep[1] - 1] not in stopWords\n",
    "                        dep_tag.append(dep[0])\n",
    "                        dep_dir.append(2)\n",
    "                    else:\n",
    "                        dep_tag.append('<pad>')\n",
    "                        dep_dir.append(0)\n",
    "                    dep_idx.append(dep[1] - 1)\n",
    "\n",
    "    # 2 hop\n",
    "    if add_2hop:\n",
    "        dep_idx_cp = dep_idx\n",
    "        for i in dep_idx_cp:\n",
    "            for dep in dependencies:\n",
    "                # connect to i, not a punct\n",
    "                if i == dep[1] - 1 and str(dep[0]) != 'punct':\n",
    "                    # not root, not aspect\n",
    "                    if (dep[2] - 1 < as_start or dep[2] - 1 >= as_end) and dep[2] != 0:\n",
    "                        if dep[2]-1 not in dep_idx:\n",
    "                            dep_tag.append(dep[0])\n",
    "                            dep_idx.append(dep[2] - 1)\n",
    "                # connect to i, not a punct\n",
    "                elif i == dep[2] - 1 and str(dep[0]) != 'punct':\n",
    "                    # not root, not aspect\n",
    "                    if (dep[1] - 1 < as_start or dep[1] - 1 >= as_end) and dep[1] != 0:\n",
    "                        if dep[1]-1 not in dep_idx:\n",
    "                            dep_tag.append(dep[0])\n",
    "                            dep_idx.append(dep[1] - 1)\n",
    "    if add_non_connect:\n",
    "        for idx, token in enumerate(tokens):\n",
    "            if idx not in dep_idx and (idx < as_start or idx >= as_end):\n",
    "                dep_tag.append('non-connect')\n",
    "                dep_dir.append(0)\n",
    "                dep_idx.append(idx)\n",
    "\n",
    "    # add aspect and index, to make sure length matches len(tokens)\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if idx not in dep_idx:\n",
    "            dep_tag.append('<pad>')\n",
    "            dep_dir.append(0)\n",
    "            dep_idx.append(idx)\n",
    "\n",
    "    index = [i[0] for i in sorted(enumerate(dep_idx), key=lambda x:x[1])]\n",
    "    dep_tag = [dep_tag[i] for i in index]\n",
    "    dep_idx = [dep_idx[i] for i in index]\n",
    "    dep_dir = [dep_dir[i] for i in index]\n",
    "\n",
    "    assert len(tokens) == len(dep_idx), 'length wrong'\n",
    "    return dep_tag, dep_idx, dep_dir\n",
    "\n",
    "def get_rolled_and_unrolled_data(input_data, args):\n",
    "    '''\n",
    "    In input_data, each sentence could have multiple aspects with different sentiments.\n",
    "    Our method treats each sentence with one aspect at a time, so even for\n",
    "    multi-aspect-multi-sentiment sentences, we will unroll them to single aspect sentence.\n",
    "\n",
    "    Perform reshape_dependency_tree to each sentence with aspect\n",
    "\n",
    "    return:\n",
    "        all_rolled:\n",
    "                a list of dict\n",
    "                    {sentence, tokens, pos_tags, pos_class, aspects(list of aspects), sentiments(list of sentiments)\n",
    "                    froms, tos, dep_tags, dep_index, dependencies}\n",
    "        all_unrolled:\n",
    "                unrolled, with aspect(single), sentiment(single) and so on...\n",
    "        mixed_rolled:\n",
    "                Multiple aspects and multiple sentiments, ROLLED.\n",
    "        mixed_unrolled:\n",
    "                Multiple aspects and multiple sentiments, UNROLLED.\n",
    "    '''\n",
    "    # A hand-picked set of part of speech tags that we see contributes to ABSA.\n",
    "    opinionated_tags = ['JJ', 'JJR', 'JJS', 'RB', 'RBR',\n",
    "                        'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "    all_rolled = []\n",
    "    all_unrolled = []\n",
    "    mixed_rolled = []\n",
    "    mixed_unrolled = []\n",
    "\n",
    "    unrolled = []\n",
    "    mixed = []\n",
    "    unrolled_ours = []\n",
    "    mixed_ours = []\n",
    "\n",
    "    # Make sure the tree is successfully built.\n",
    "    zero_dep_counter = 0\n",
    "\n",
    "    # Sentiment counters\n",
    "    total_counter = defaultdict(int)\n",
    "    mixed_counter = defaultdict(int)\n",
    "    sentiments_lookup = {'negative': 0, 'positive': 1, 'neutral': 2}\n",
    "\n",
    "    logger.info('*** Start processing data(unrolling and reshaping) ***')\n",
    "    tree_samples = []\n",
    "    # for seeking 'but' examples\n",
    "    for e in input_data:\n",
    "        e['tokens'] = [x.lower() for x in e['tokens']]\n",
    "        aspects = []\n",
    "        sentiments = []\n",
    "        froms = []\n",
    "        tos = []\n",
    "        dep_tags = []\n",
    "        dep_index = []\n",
    "        dep_dirs = []\n",
    "\n",
    "        # Classify based on POS-tags\n",
    "\n",
    "        pos_class = e['tags']\n",
    "\n",
    "        # Iterate through aspects in a sentence and reshape the dependency tree.\n",
    "        for i in range(len(e['aspect_sentiment'])):\n",
    "            aspect = e['aspect_sentiment'][i][0].lower()\n",
    "            # We would tokenize the aspect while at it.\n",
    "            aspect = word_tokenize(aspect)\n",
    "            sentiment = sentiments_lookup[e['aspect_sentiment'][i][1]]\n",
    "            frm = e['from_to'][i][0]\n",
    "            to = e['from_to'][i][1]\n",
    "\n",
    "            aspects.append(aspect)\n",
    "            sentiments.append(sentiment)\n",
    "            froms.append(frm)\n",
    "            tos.append(to)\n",
    "\n",
    "            # Center on the aspect.\n",
    "            dep_tag, dep_idx, dep_dir = reshape_dependency_tree_new(frm, to, e['dependencies'],\n",
    "                                                       multi_hop=args.multi_hop, add_non_connect=args.add_non_connect, tokens=e['tokens'], max_hop=args.max_hop)\n",
    "\n",
    "            # Because of tokenizer differences, aspect opsitions are off, so we find the index and try again.\n",
    "            if len(dep_tag) == 0:\n",
    "                zero_dep_counter += 1\n",
    "                as_sent = e['aspect_sentiment'][i][0].split()\n",
    "                as_start = e['tokens'].index(as_sent[0])\n",
    "                # print(e['tokens'], e['aspect_sentiment'], e['dependencies'],as_sent[0])\n",
    "                as_end = e['tokens'].index(\n",
    "                    as_sent[-1]) if len(as_sent) > 1 else as_start + 1\n",
    "                print(\"Debugging: as_start as_end \", as_start, as_end)\n",
    "                dep_tag, dep_idx, dep_dir = reshape_dependency_tree_new(as_start, as_end, e['dependencies'],\n",
    "                                                           multi_hop=args.multi_hop, add_non_connect=args.add_non_connect, tokens=e['tokens'], max_hop=args.max_hop)\n",
    "                if len(dep_tag) == 0:  # for debugging\n",
    "                    print(\"Debugging: zero_dep\",\n",
    "                          e['aspect_sentiment'][i][0], e['tokens'])\n",
    "                    print(\"Debugging: \". e['dependencies'])\n",
    "                else:\n",
    "                    zero_dep_counter -= 1\n",
    "\n",
    "            dep_tags.append(dep_tag)\n",
    "            dep_index.append(dep_idx)\n",
    "            dep_dirs.append(dep_dir)\n",
    "\n",
    "            total_counter[e['aspect_sentiment'][i][1]] += 1\n",
    "\n",
    "            # Unrolling\n",
    "            all_unrolled.append(\n",
    "                {'sentence': e['tokens'], 'tags': e['tags'], 'pos_class': pos_class, 'aspect': aspect, 'sentiment': sentiment,\n",
    "                    'predicted_dependencies': e['predicted_dependencies'], 'predicted_heads': e['predicted_heads'],\n",
    "                 'from': frm, 'to': to, 'dep_tag': dep_tag, 'dep_idx': dep_idx, 'dep_dir':dep_dir,'dependencies': e['dependencies']})\n",
    "\n",
    "\n",
    "        # All sentences with multiple aspects and sentiments rolled.\n",
    "        all_rolled.append(\n",
    "            {'sentence': e['tokens'], 'tags': e['tags'], 'pos_class': pos_class, 'aspects': aspects, 'sentiments': sentiments,\n",
    "             'from': froms, 'to': tos, 'dep_tags': dep_tags, 'dep_index': dep_index, 'dependencies': e['dependencies']})\n",
    "\n",
    "        # Ignore sentences with single aspect or no aspect\n",
    "        if len(e['aspect_sentiment']) and len(set(map(lambda x: x[1], e['aspect_sentiment']))) > 1:\n",
    "            mixed_rolled.append(\n",
    "                {'sentence': e['tokens'], 'tags': e['tags'], 'pos_class': pos_class, 'aspects': aspects, 'sentiments': sentiments,\n",
    "                 'from': froms, 'to': tos, 'dep_tags': dep_tags, 'dep_index': dep_index, 'dependencies': e['dependencies']})\n",
    "\n",
    "            # Unrolling\n",
    "            for i, as_sent in enumerate(e['aspect_sentiment']):\n",
    "                mixed_counter[as_sent[1]] += 1\n",
    "                mixed_unrolled.append(\n",
    "                    {'sentence': e['tokens'], 'tags': e['tags'], 'pos_class': pos_class, 'aspect': aspects[i], 'sentiment': sentiments[i],\n",
    "                     'from': froms[i], 'to': tos[i], 'dep_tag': dep_tags[i], 'dep_idx': dep_index[i], 'dependencies': e['dependencies']})\n",
    "\n",
    "\n",
    "    logger.info('Total sentiment counter: %s', total_counter)\n",
    "    logger.info('Multi-Aspect-Multi-Sentiment counter: %s', mixed_counter)\n",
    "\n",
    "    return all_rolled, all_unrolled, mixed_rolled, mixed_unrolled\n",
    "\n",
    "\n",
    "def load_and_cache_vocabs(data, args):\n",
    "    '''\n",
    "    Build vocabulary of words, part of speech tags, dependency tags and cache them.\n",
    "    Load glove embedding if needed.\n",
    "    '''\n",
    "    pkls_path = os.path.join(args.output_dir, 'pkls')\n",
    "    if not os.path.exists(pkls_path):\n",
    "        os.makedirs(pkls_path)\n",
    "\n",
    "\n",
    "    word_vocab = None\n",
    "    word_vecs = None\n",
    "\n",
    "    # Build vocab of dependency tags\n",
    "    cached_dep_tag_vocab_file = os.path.join(\n",
    "        pkls_path, 'cached_{}_dep_tag_vocab.pkl'.format(args.dataset_name))\n",
    "    if os.path.exists(cached_dep_tag_vocab_file):\n",
    "        logger.info('Loading vocab of dependency tags from %s',\n",
    "                    cached_dep_tag_vocab_file)\n",
    "        with open(cached_dep_tag_vocab_file, 'rb') as f:\n",
    "            dep_tag_vocab = pickle.load(f)\n",
    "    else:\n",
    "        logger.info('Creating vocab of dependency tags.')\n",
    "        dep_tag_vocab = build_dep_tag_vocab(data, min_freq=0)\n",
    "        logger.info('Saving dependency tags  vocab, size: %s, to file %s',\n",
    "                    dep_tag_vocab['len'], cached_dep_tag_vocab_file)\n",
    "        with open(cached_dep_tag_vocab_file, 'wb') as f:\n",
    "            pickle.dump(dep_tag_vocab, f, -1)\n",
    "\n",
    "    # Build vocab of part of speech tags.\n",
    "    cached_pos_tag_vocab_file = os.path.join(\n",
    "        pkls_path, 'cached_{}_pos_tag_vocab.pkl'.format(args.dataset_name))\n",
    "    if os.path.exists(cached_pos_tag_vocab_file):\n",
    "        logger.info('Loading vocab of dependency tags from %s',\n",
    "                    cached_pos_tag_vocab_file)\n",
    "        with open(cached_pos_tag_vocab_file, 'rb') as f:\n",
    "            pos_tag_vocab = pickle.load(f)\n",
    "    else:\n",
    "        logger.info('Creating vocab of dependency tags.')\n",
    "        pos_tag_vocab = build_pos_tag_vocab(data, min_freq=0)\n",
    "        logger.info('Saving dependency tags  vocab, size: %s, to file %s',\n",
    "                    pos_tag_vocab['len'], cached_pos_tag_vocab_file)\n",
    "        with open(cached_pos_tag_vocab_file, 'wb') as f:\n",
    "            pickle.dump(pos_tag_vocab, f, -1)\n",
    "\n",
    "    return word_vecs, word_vocab, dep_tag_vocab, pos_tag_vocab\n",
    "\n",
    "\n",
    "\n",
    "def _default_unk_index():\n",
    "    return 1\n",
    "\n",
    "\n",
    "def build_text_vocab(data, vocab_size=100000, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for d in data:\n",
    "        s = d['sentence']\n",
    "        counter.update(s)\n",
    "\n",
    "    itos = ['[PAD]', '[UNK]']\n",
    "    min_freq = max(min_freq, 1)\n",
    "\n",
    "    # sort by frequency, then alphabetically\n",
    "    words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "    words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "    for word, freq in words_and_frequencies:\n",
    "        if freq < min_freq or len(itos) == vocab_size:\n",
    "            break\n",
    "        itos.append(word)\n",
    "    # stoi is simply a reverse dict for itos\n",
    "    stoi = defaultdict(_default_unk_index)\n",
    "    stoi.update({tok: i for i, tok in enumerate(itos)})\n",
    "\n",
    "    return {'itos': itos, 'stoi': stoi, 'len': len(itos)}\n",
    "\n",
    "\n",
    "def build_pos_tag_vocab(data, vocab_size=1000, min_freq=1):\n",
    "    \"\"\"\n",
    "    Part of speech tags vocab.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for d in data:\n",
    "        tags = d['tags']\n",
    "        counter.update(tags)\n",
    "\n",
    "    itos = ['<pad>']\n",
    "    min_freq = max(min_freq, 1)\n",
    "\n",
    "    # sort by frequency, then alphabetically\n",
    "    words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "    words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "    for word, freq in words_and_frequencies:\n",
    "        if freq < min_freq or len(itos) == vocab_size:\n",
    "            break\n",
    "        itos.append(word)\n",
    "    # stoi is simply a reverse dict for itos\n",
    "    stoi = defaultdict()\n",
    "    stoi.update({tok: i for i, tok in enumerate(itos)})\n",
    "\n",
    "    return {'itos': itos, 'stoi': stoi, 'len': len(itos)}\n",
    "\n",
    "\n",
    "# def build_dep_tag_vocab_energy():  # 47 in total, all tags plus pad and non-connect\n",
    "#     '''\n",
    "#     biaffine dep_tag Vocab : {0: 'punct', 1: 'prep', 2: 'pobj', 3: 'det', 4: 'nn',\n",
    "#         5: 'nsubj', 6: 'amod', 7: 'root', 8: 'dobj', 9: 'aux', 10: 'advmod', 11: 'conj',\n",
    "#         12: 'cc', 13: 'num', 14: 'poss', 15: 'ccomp', 16: 'dep', 17: 'xcomp', 18: 'mark',\n",
    "#         19: 'cop', 20: 'number', 21: 'possessive', 22: 'rcmod', 23: 'auxpass', 24: 'appos',\n",
    "#         25: 'nsubjpass', 26: 'advcl', 27: 'partmod', 28: 'pcomp', 29: 'neg', 30: 'tmod',\n",
    "#         31: 'quantmod', 32: 'npadvmod', 33: 'prt', 34: 'infmod', 35: 'parataxis',\n",
    "#         36: 'mwe', 37: 'expl', 38: 'acomp', 39: 'iobj', 40: 'csubj', 41: 'predet',\n",
    "#         42: 'preconj', 43: 'discourse', 44: 'csubjpass'}\n",
    "#     This is used in energy case.\n",
    "#     '''\n",
    "#     head_tags = {0: 'punct', 1: 'prep', 2: 'pobj', 3: 'det', 4: 'nn', 5: 'nsubj', 6: 'amod', 7: 'root', 8: 'dobj', 9: 'aux', 10: 'advmod', 11: 'conj', 12: 'cc', 13: 'num', 14: 'poss', 15: 'ccomp', 16: 'dep', 17: 'xcomp', 18: 'mark', 19: 'cop', 20: 'number', 21: 'possessive', 22: 'rcmod', 23: 'auxpass', 24: 'appos',\n",
    "#                  25: 'nsubjpass', 26: 'advcl', 27: 'partmod', 28: 'pcomp', 29: 'neg', 30: 'tmod', 31: 'quantmod', 32: 'npadvmod', 33: 'prt', 34: 'infmod', 35: 'parataxis', 36: 'mwe', 37: 'expl', 38: 'acomp', 39: 'iobj', 40: 'csubj', 41: 'predet', 42: 'preconj', 43: 'discourse', 44: 'csubjpass', 45: '<pad>', 46: 'non-connect'}\n",
    "#     itos = [head_tags[i] for i in range(len(head_tags))]\n",
    "#     stoi = defaultdict()\n",
    "#     stoi.update({tok: i for i, tok in enumerate(itos)})\n",
    "#     return {'itos': itos, 'stoi': stoi, 'len': len(itos)}\n",
    "\n",
    "\n",
    "def build_dep_tag_vocab(data, vocab_size=1000, min_freq=0):\n",
    "    counter = Counter()\n",
    "    for d in data:\n",
    "        tags = d['dep_tag']\n",
    "        counter.update(tags)\n",
    "\n",
    "    itos = ['<pad>', '<unk>']\n",
    "    min_freq = max(min_freq, 1)\n",
    "\n",
    "    # sort by frequency, then alphabetically\n",
    "    words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "    words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "    for word, freq in words_and_frequencies:\n",
    "        if freq < min_freq or len(itos) == vocab_size:\n",
    "            break\n",
    "        if word == '<pad>':\n",
    "            continue\n",
    "        itos.append(word)\n",
    "    # stoi is simply a reverse dict for itos\n",
    "    stoi = defaultdict(_default_unk_index)\n",
    "    stoi.update({tok: i for i, tok in enumerate(itos)})\n",
    "\n",
    "    return {'itos': itos, 'stoi': stoi, 'len': len(itos)}\n",
    "\n",
    "\n",
    "class ASBA_Depparsed_Dataset(Dataset):\n",
    "    '''\n",
    "    Convert examples to features, numericalize text to ids.\n",
    "    data:\n",
    "        -list of dict:\n",
    "            keys: sentence, tags, pos_class, aspect, sentiment,\n",
    "                predicted_dependencies, predicted_heads,\n",
    "                from, to, dep_tag, dep_idx, dependencies, dep_dir\n",
    "\n",
    "    After processing,\n",
    "    data:\n",
    "        sentence\n",
    "        tags\n",
    "        pos_class\n",
    "        aspect\n",
    "        sentiment\n",
    "        from\n",
    "        to\n",
    "        dep_tag\n",
    "        dep_idx\n",
    "        dep_dir\n",
    "        predicted_dependencies_ids\n",
    "        predicted_heads\n",
    "        dependencies\n",
    "        sentence_ids\n",
    "        aspect_ids\n",
    "        tag_ids\n",
    "        dep_tag_ids\n",
    "        text_len\n",
    "        aspect_len\n",
    "        if bert:\n",
    "            input_ids\n",
    "            word_indexer\n",
    "\n",
    "    Return from getitem:\n",
    "        sentence_ids\n",
    "        aspect_ids\n",
    "        dep_tag_ids\n",
    "        dep_dir_ids\n",
    "        pos_class\n",
    "        text_len\n",
    "        aspect_len\n",
    "        sentiment\n",
    "        deprel\n",
    "        dephead\n",
    "        aspect_position\n",
    "        if bert:\n",
    "            input_ids\n",
    "            word_indexer\n",
    "            input_aspect_ids\n",
    "            aspect_indexer\n",
    "        or:\n",
    "            input_cat_ids\n",
    "            segment_ids\n",
    "    '''\n",
    "\n",
    "    def __init__(self, data, args, word_vocab, dep_tag_vocab, pos_tag_vocab):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "        self.word_vocab = word_vocab\n",
    "        self.dep_tag_vocab = dep_tag_vocab\n",
    "        self.pos_tag_vocab = pos_tag_vocab\n",
    "\n",
    "        self.convert_features()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        items = e['dep_tag_ids'], e['pos_class'], e['text_len'], e['aspect_len'], e['sentiment'],\\\n",
    "            e['dep_rel_ids'], e['predicted_heads'], e['aspect_position'], e['dep_dir_ids']\n",
    "\n",
    "        bert_items = e['input_ids'], e['word_indexer'], e['input_aspect_ids'], e['aspect_indexer'], e['input_cat_ids'], e['segment_ids']\n",
    "        # segment_id\n",
    "        items_tensor = tuple(torch.tensor(t) for t in bert_items)\n",
    "        items_tensor += tuple(torch.tensor(t) for t in items)\n",
    "        return items_tensor\n",
    "\n",
    "    def convert_features_bert(self, i):\n",
    "        \"\"\"\n",
    "        BERT features.\n",
    "        convert sentence to feature. \n",
    "        \"\"\"\n",
    "        cls_token = \"[CLS]\"\n",
    "        sep_token = \"[SEP]\"\n",
    "        pad_token = 0\n",
    "        # tokenizer = self.args.tokenizer\n",
    "\n",
    "        tokens = []\n",
    "        word_indexer = []\n",
    "        aspect_tokens = []\n",
    "        aspect_indexer = []\n",
    "\n",
    "        for word in self.data[i]['sentence']:\n",
    "            word_tokens = self.args.tokenizer.tokenize(word)\n",
    "            token_idx = len(tokens)\n",
    "            tokens.extend(word_tokens)\n",
    "            # word_indexer is for indexing after bert, feature back to the length of original length.\n",
    "            word_indexer.append(token_idx)\n",
    "\n",
    "        # aspect\n",
    "        for word in self.data[i]['aspect']:\n",
    "            word_aspect_tokens = self.args.tokenizer.tokenize(word)\n",
    "            token_idx = len(aspect_tokens)\n",
    "            aspect_tokens.extend(word_aspect_tokens)\n",
    "            aspect_indexer.append(token_idx)\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids:   0   0   0   0  0     0   0\n",
    "\n",
    "        tokens = [cls_token] + tokens + [sep_token]\n",
    "        aspect_tokens = [cls_token] + aspect_tokens + [sep_token]\n",
    "        word_indexer = [i+1 for i in word_indexer]\n",
    "        aspect_indexer = [i+1 for i in aspect_indexer]\n",
    "\n",
    "        input_ids = self.args.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_aspect_ids = self.args.tokenizer.convert_tokens_to_ids(\n",
    "            aspect_tokens)\n",
    "\n",
    "        # check len of word_indexer equals to len of sentence.\n",
    "        assert len(word_indexer) == len(self.data[i]['sentence'])\n",
    "        assert len(aspect_indexer) == len(self.data[i]['aspect'])\n",
    "\n",
    "        # THE STEP:Zero-pad up to the sequence length, save to collate_fn.\n",
    "\n",
    "        input_cat_ids = input_ids + input_aspect_ids[1:]\n",
    "        segment_ids = [0] * len(input_ids) + [1] * len(input_aspect_ids[1:])\n",
    "\n",
    "        self.data[i]['input_cat_ids'] = input_cat_ids\n",
    "        self.data[i]['segment_ids'] = segment_ids\n",
    "        self.data[i]['input_ids'] = input_ids\n",
    "        self.data[i]['word_indexer'] = word_indexer\n",
    "        self.data[i]['input_aspect_ids'] = input_aspect_ids\n",
    "        self.data[i]['aspect_indexer'] = aspect_indexer\n",
    "\n",
    "    def convert_features(self):\n",
    "        '''\n",
    "        Convert sentence, aspects, pos_tags, dependency_tags to ids.\n",
    "        '''\n",
    "        for i in range(len(self.data)):\n",
    "\n",
    "            self.convert_features_bert(i)\n",
    "\n",
    "            self.data[i]['text_len'] = len(self.data[i]['sentence'])\n",
    "            self.data[i]['aspect_position'] = [0] * self.data[i]['text_len']\n",
    "            try:  # find the index of aspect in sentence\n",
    "                for j in range(self.data[i]['from'], self.data[i]['to']):\n",
    "                    self.data[i]['aspect_position'][j] = 1\n",
    "            except:\n",
    "                for term in self.data[i]['aspect']:\n",
    "                    self.data[i]['aspect_position'][self.data[i]\n",
    "                                                    ['sentence'].index(term)] = 1\n",
    "\n",
    "            self.data[i]['dep_tag_ids'] = [self.dep_tag_vocab['stoi'][w]\n",
    "                                           for w in self.data[i]['dep_tag']]\n",
    "            self.data[i]['dep_dir_ids'] = [idx\n",
    "                                           for idx in self.data[i]['dep_dir']]\n",
    "            self.data[i]['pos_class'] = [self.pos_tag_vocab['stoi'][w]\n",
    "                                             for w in self.data[i]['tags']]\n",
    "            self.data[i]['aspect_len'] = len(self.data[i]['aspect'])\n",
    "\n",
    "            self.data[i]['dep_rel_ids'] = [self.dep_tag_vocab['stoi'][r]\n",
    "                                           for r in self.data[i]['predicted_dependencies']]\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    '''\n",
    "    Pad sentence and aspect in a batch.\n",
    "    Sort the sentences based on length.\n",
    "    Turn all into tensors.\n",
    "    '''\n",
    "    sentence_ids, aspect_ids, dep_tag_ids, pos_class, text_len, aspect_len, sentiment, dep_rel_ids, dep_heads, aspect_positions, dep_dir_ids = zip(\n",
    "        *batch)  # from Dataset.__getitem__()\n",
    "    text_len = torch.tensor(text_len)\n",
    "    aspect_len = torch.tensor(aspect_len)\n",
    "    sentiment = torch.tensor(sentiment)\n",
    "\n",
    "    # Pad sequences.\n",
    "    sentence_ids = pad_sequence(\n",
    "        sentence_ids, batch_first=True, padding_value=0)\n",
    "    aspect_ids = pad_sequence(aspect_ids, batch_first=True, padding_value=0)\n",
    "    aspect_positions = pad_sequence(\n",
    "        aspect_positions, batch_first=True, padding_value=0)\n",
    "\n",
    "    dep_tag_ids = pad_sequence(dep_tag_ids, batch_first=True, padding_value=0)\n",
    "    dep_dir_ids = pad_sequence(dep_dir_ids, batch_first=True, padding_value=0)\n",
    "    pos_class = pad_sequence(pos_class, batch_first=True, padding_value=0)\n",
    "\n",
    "    dep_rel_ids = pad_sequence(dep_rel_ids, batch_first=True, padding_value=0)\n",
    "    dep_heads = pad_sequence(dep_heads, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Sort all tensors based on text len.\n",
    "    _, sorted_idx = text_len.sort(descending=True)\n",
    "    sentence_ids = sentence_ids[sorted_idx]\n",
    "    aspect_ids = aspect_ids[sorted_idx]\n",
    "    aspect_positions = aspect_positions[sorted_idx]\n",
    "    dep_tag_ids = dep_tag_ids[sorted_idx]\n",
    "    dep_dir_ids = dep_dir_ids[sorted_idx]\n",
    "    pos_class = pos_class[sorted_idx]\n",
    "    text_len = text_len[sorted_idx]\n",
    "    aspect_len = aspect_len[sorted_idx]\n",
    "    sentiment = sentiment[sorted_idx]\n",
    "    dep_rel_ids = dep_rel_ids[sorted_idx]\n",
    "    dep_heads = dep_heads[sorted_idx]\n",
    "\n",
    "    return sentence_ids, aspect_ids, dep_tag_ids, pos_class, text_len, aspect_len, sentiment, dep_rel_ids, dep_heads, aspect_positions, dep_dir_ids\n",
    "\n",
    "\n",
    "def my_collate_elmo(batch):\n",
    "    '''\n",
    "    Pad sentence and aspect in a batch.\n",
    "    Sort the sentences based on length.\n",
    "    Turn all into tensors.\n",
    "    The difference with my_collate is just padding method with sentence_ids and aspect_ids.\n",
    "    '''\n",
    "    sentence_ids, aspect_ids, dep_tag_ids, pos_class, text_len, aspect_len, sentiment, dep_rel_ids, dep_heads, aspect_positions, dep_dir_ids = zip(\n",
    "        *batch)\n",
    "    text_len = torch.tensor(text_len)\n",
    "    aspect_len = torch.tensor(aspect_len)\n",
    "    sentiment = torch.tensor(sentiment)\n",
    "\n",
    "    # Pad sequences.\n",
    "    sentence_ids = batch_to_ids(sentence_ids)\n",
    "    aspect_ids = batch_to_ids(aspect_ids)\n",
    "\n",
    "    aspect_positions = pad_sequence(\n",
    "        aspect_positions, batch_first=True, padding_value=0)\n",
    "\n",
    "    dep_tag_ids = pad_sequence(dep_tag_ids, batch_first=True, padding_value=0)\n",
    "    dep_dir_ids = pad_sequence(dep_dir_ids, batch_first=True, padding_value=0)\n",
    "    pos_class = pad_sequence(pos_class, batch_first=True, padding_value=0)\n",
    "\n",
    "    dep_rel_ids = pad_sequence(dep_rel_ids, batch_first=True, padding_value=0)\n",
    "    dep_heads = pad_sequence(dep_heads, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Sort all tensors based on text len.\n",
    "    _, sorted_idx = text_len.sort(descending=True)\n",
    "    sentence_ids = sentence_ids[sorted_idx]\n",
    "    aspect_ids = aspect_ids[sorted_idx]\n",
    "    aspect_positions = aspect_positions[sorted_idx]\n",
    "    dep_tag_ids = dep_tag_ids[sorted_idx]\n",
    "    dep_dir_ids = dep_dir_ids[sorted_idx]\n",
    "    pos_class = pos_class[sorted_idx]\n",
    "    text_len = text_len[sorted_idx]\n",
    "    aspect_len = aspect_len[sorted_idx]\n",
    "    sentiment = sentiment[sorted_idx]\n",
    "    dep_rel_ids = dep_rel_ids[sorted_idx]\n",
    "    dep_heads = dep_heads[sorted_idx]\n",
    "\n",
    "    return sentence_ids, aspect_ids, dep_tag_ids, pos_class, text_len, aspect_len, sentiment, dep_rel_ids, dep_heads, aspect_positions, dep_dir_ids\n",
    "\n",
    "\n",
    "# 11/7\n",
    "def my_collate_pure_bert(batch):\n",
    "    '''\n",
    "    Pad sentence and aspect in a batch.\n",
    "    Sort the sentences based on length.\n",
    "    Turn all into tensors.\n",
    "\n",
    "    Process bert feature\n",
    "    Pure Bert: cat text and aspect, cls to predict.\n",
    "    Test indexing while at it?\n",
    "    '''\n",
    "    # sentence_ids, aspect_ids\n",
    "    input_cat_ids, segment_ids, dep_tag_ids, pos_class, text_len, aspect_len, sentiment, dep_rel_ids, dep_heads, aspect_positions, dep_dir_ids = zip(\n",
    "        *batch)  # from Dataset.__getitem__()\n",
    "\n",
    "    text_len = torch.tensor(text_len)\n",
    "    aspect_len = torch.tensor(aspect_len)\n",
    "    sentiment = torch.tensor(sentiment)\n",
    "\n",
    "    # Pad sequences.\n",
    "    input_cat_ids = pad_sequence(\n",
    "        input_cat_ids, batch_first=True, padding_value=0)\n",
    "    segment_ids = pad_sequence(segment_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "    aspect_positions = pad_sequence(\n",
    "        aspect_positions, batch_first=True, padding_value=0)\n",
    "\n",
    "    dep_tag_ids = pad_sequence(dep_tag_ids, batch_first=True, padding_value=0)\n",
    "    dep_dir_ids = pad_sequence(dep_dir_ids, batch_first=True, padding_value=0)\n",
    "    pos_class = pad_sequence(pos_class, batch_first=True, padding_value=0)\n",
    "\n",
    "    dep_rel_ids = pad_sequence(dep_rel_ids, batch_first=True, padding_value=0)\n",
    "    dep_heads = pad_sequence(dep_heads, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Sort all tensors based on text len.\n",
    "    _, sorted_idx = text_len.sort(descending=True)\n",
    "    input_cat_ids = input_cat_ids[sorted_idx]\n",
    "    segment_ids = segment_ids[sorted_idx]\n",
    "    aspect_positions = aspect_positions[sorted_idx]\n",
    "    dep_tag_ids = dep_tag_ids[sorted_idx]\n",
    "\n",
    "    dep_dir_ids = dep_dir_ids[sorted_idx]\n",
    "    pos_class = pos_class[sorted_idx]\n",
    "    text_len = text_len[sorted_idx]\n",
    "    aspect_len = aspect_len[sorted_idx]\n",
    "    sentiment = sentiment[sorted_idx]\n",
    "    dep_rel_ids = dep_rel_ids[sorted_idx]\n",
    "    dep_heads = dep_heads[sorted_idx]\n",
    "\n",
    "    return input_cat_ids, segment_ids, dep_tag_ids, pos_class, text_len, aspect_len, sentiment, dep_rel_ids, dep_heads, aspect_positions, dep_dir_ids\n",
    "\n",
    "\n",
    "def my_collate_bert(batch):\n",
    "    '''\n",
    "    Pad sentence and aspect in a batch.\n",
    "    Sort the sentences based on length.\n",
    "    Turn all into tensors.\n",
    "\n",
    "    Process bert feature\n",
    "    '''\n",
    "    input_ids, word_indexer, input_aspect_ids, aspect_indexer,input_cat_ids,segment_ids,  dep_tag_ids, pos_class, text_len, aspect_len, sentiment, dep_rel_ids, dep_heads, aspect_positions, dep_dir_ids= zip(\n",
    "        *batch)\n",
    "    text_len = torch.tensor(text_len)\n",
    "    aspect_len = torch.tensor(aspect_len)\n",
    "    sentiment = torch.tensor(sentiment)\n",
    "\n",
    "    # Pad sequences.\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    input_aspect_ids = pad_sequence(input_aspect_ids, batch_first=True, padding_value=0)\n",
    "    input_cat_ids = pad_sequence(input_cat_ids, batch_first=True, padding_value=0)\n",
    "    segment_ids = pad_sequence(segment_ids, batch_first=True, padding_value =0)\n",
    "    # indexer are padded with 1, for ...\n",
    "    word_indexer = pad_sequence(word_indexer, batch_first=True, padding_value=1)\n",
    "    aspect_indexer = pad_sequence(aspect_indexer, batch_first=True, padding_value=1)\n",
    "\n",
    "    aspect_positions = pad_sequence(\n",
    "        aspect_positions, batch_first=True, padding_value=0)\n",
    "\n",
    "    dep_tag_ids = pad_sequence(dep_tag_ids, batch_first=True, padding_value=0)\n",
    "    dep_dir_ids = pad_sequence(dep_dir_ids, batch_first=True, padding_value=0)\n",
    "    pos_class = pad_sequence(pos_class, batch_first=True, padding_value=0)\n",
    "\n",
    "    dep_rel_ids = pad_sequence(dep_rel_ids, batch_first=True, padding_value=0)\n",
    "    dep_heads = pad_sequence(dep_heads, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Sort all tensors based on text len.\n",
    "    _, sorted_idx = text_len.sort(descending=True)\n",
    "    input_ids = input_ids[sorted_idx]\n",
    "    input_aspect_ids = input_aspect_ids[sorted_idx]\n",
    "    word_indexer = word_indexer[sorted_idx]\n",
    "    aspect_indexer = aspect_indexer[sorted_idx]\n",
    "    input_cat_ids = input_cat_ids[sorted_idx]\n",
    "    segment_ids = segment_ids[sorted_idx]\n",
    "    aspect_positions = aspect_positions[sorted_idx]\n",
    "    dep_tag_ids = dep_tag_ids[sorted_idx]\n",
    "    dep_dir_ids = dep_dir_ids[sorted_idx]\n",
    "    pos_class = pos_class[sorted_idx]\n",
    "    text_len = text_len[sorted_idx]\n",
    "    aspect_len = aspect_len[sorted_idx]\n",
    "    sentiment = sentiment[sorted_idx]\n",
    "    dep_rel_ids = dep_rel_ids[sorted_idx]\n",
    "    dep_heads = dep_heads[sorted_idx]\n",
    "\n",
    "    return input_ids, word_indexer, input_aspect_ids, aspect_indexer,input_cat_ids,segment_ids, dep_tag_ids, pos_class, text_len, aspect_len, sentiment, dep_rel_ids, dep_heads, aspect_positions, dep_dir_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aspect_Bert_GAT(nn.Module):\n",
    "    '''\n",
    "    R-GAT with bert\n",
    "    '''\n",
    "\n",
    "    def __init__(self, args, dep_tag_num, pos_tag_num):\n",
    "        super(Aspect_Bert_GAT, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        # Bert\n",
    "        config = BertConfig.from_pretrained(args.bert_model_dir)\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            args.bert_model_dir, config=config, from_tf =False)\n",
    "        self.dropout_bert = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        args.embedding_dim = config.hidden_size  # 768\n",
    "\n",
    "        if args.highway:\n",
    "            self.highway_dep = Highway(args.num_layers, args.embedding_dim)\n",
    "            self.highway = Highway(args.num_layers, args.embedding_dim)\n",
    "\n",
    "\n",
    "        gcn_input_dim = args.embedding_dim\n",
    "\n",
    "        # GAT\n",
    "        self.gat_dep = [RelationAttention(in_dim=args.embedding_dim).to(args.device) for i in range(args.num_heads)]\n",
    "\n",
    "\n",
    "        self.dep_embed = nn.Embedding(dep_tag_num, args.embedding_dim)\n",
    "\n",
    "        last_hidden_size = args.embedding_dim * 2\n",
    "        layers = [\n",
    "            nn.Linear(last_hidden_size, args.final_hidden_size), nn.ReLU()]\n",
    "        for _ in range(args.num_mlps - 1):\n",
    "            layers += [nn.Linear(args.final_hidden_size,\n",
    "                                 args.final_hidden_size), nn.ReLU()]\n",
    "        self.fcs = nn.Sequential(*layers)\n",
    "        self.fc_final = nn.Linear(args.final_hidden_size, args.num_classes)\n",
    "\n",
    "    def forward(self, input_ids, input_aspect_ids, word_indexer, aspect_indexer,input_cat_ids,segment_ids, pos_class, dep_tags, text_len, aspect_len, dep_rels, dep_heads, aspect_position, dep_dirs):\n",
    "        fmask = (torch.ones_like(word_indexer) != word_indexer).float()  # (Nï¼ŒL)\n",
    "        fmask[:,0] = 1\n",
    "        outputs = self.bert(input_cat_ids, token_type_ids = segment_ids)\n",
    "        feature_output = outputs[0] # (N, L, D)\n",
    "        pool_out = outputs[1] #(N, D)\n",
    "\n",
    "        # index select, back to original batched size.\n",
    "        feature = torch.stack([torch.index_select(f, 0, w_i)\n",
    "                               for f, w_i in zip(feature_output, word_indexer)])\n",
    "\n",
    "        ############################################################################################\n",
    "        # do gat thing\n",
    "        dep_feature = self.dep_embed(dep_tags)\n",
    "        if self.args.highway:\n",
    "            dep_feature = self.highway_dep(dep_feature)\n",
    "\n",
    "        dep_out = [g(feature, dep_feature, fmask).unsqueeze(1) for g in self.gat_dep]  # (N, 1, D) * num_heads\n",
    "        dep_out = torch.cat(dep_out, dim=1)  # (N, H, D)\n",
    "        dep_out = dep_out.mean(dim=1)  # (N, D)\n",
    "\n",
    "\n",
    "        feature_out = torch.cat([dep_out,  pool_out], dim=1)  # (N, D')\n",
    "        # feature_out = gat_out\n",
    "        #############################################################################################\n",
    "        x = self.dropout(feature_out)\n",
    "        x = self.fcs(x)\n",
    "        logit = self.fc_final(x)\n",
    "        return logit\n",
    "\n",
    "\n",
    "def rnn_zero_state(batch_size, hidden_dim, num_layers, bidirectional=True, use_cuda=True):\n",
    "    total_layers = num_layers * 2 if bidirectional else num_layers\n",
    "    state_shape = (total_layers, batch_size, hidden_dim)\n",
    "    h0 = c0 = Variable(torch.zeros(*state_shape), requires_grad=False)\n",
    "    if use_cuda:\n",
    "        return h0.cuda(), c0.cuda()\n",
    "    else:\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from datasets import my_collate, my_collate_elmo, my_collate_pure_bert, my_collate_bert\n",
    "from transformers import AdamW\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "def get_input_from_batch(args, batch):\n",
    "    embedding_type = args.embedding_type\n",
    "    # input_ids, word_indexer, input_aspect_ids, aspect_indexer, dep_tag_ids, pos_class, text_len, aspect_len, sentiment, dep_rel_ids, dep_heads, aspect_positions\n",
    "    inputs = {  'input_ids': batch[0],\n",
    "                'input_aspect_ids': batch[2],\n",
    "                'word_indexer': batch[1],\n",
    "                'aspect_indexer': batch[3],\n",
    "                'input_cat_ids': batch[4],\n",
    "                'segment_ids': batch[5],\n",
    "                'dep_tags': batch[6],\n",
    "                'pos_class': batch[7],\n",
    "                'text_len': batch[8],\n",
    "                'aspect_len': batch[9],\n",
    "                'dep_rels': batch[11],\n",
    "                'dep_heads': batch[12],\n",
    "                'aspect_position': batch[13],\n",
    "                'dep_dirs': batch[14]}\n",
    "    labels = batch[10]\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def get_collate_fn(args):\n",
    "    embedding_type = args.embedding_type\n",
    "    return my_collate_bert\n",
    "\n",
    "\n",
    "def get_bert_optimizer(args, model):\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    # scheduler = WarmupLinearSchedule(\n",
    "    #     optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train(args, train_dataset, model, test_dataset):\n",
    "    '''Train the model'''\n",
    "    tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    collate_fn = get_collate_fn(args)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler,\n",
    "                                  batch_size=args.train_batch_size,\n",
    "                                  collate_fn=collate_fn)\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (\n",
    "            len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(\n",
    "            train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    \n",
    "\n",
    "    optimizer = get_bert_optimizer(args, model)\n",
    "\n",
    "\n",
    "    # Train\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\",\n",
    "                args.per_gpu_train_batch_size)\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\",\n",
    "                args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    all_eval_results = []\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "    set_seed(args)\n",
    "    for _ in train_iterator:\n",
    "        # epoch_iterator = tqdm(train_dataloader, desc='Iteration')\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "            inputs, labels = get_input_from_batch(args, batch)\n",
    "            logit = model(**inputs)\n",
    "            loss = F.cross_entropy(logit, labels)\n",
    "\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                # scheduler.step()  # Update learning rate schedule\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                # Log metrics\n",
    "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    results, eval_loss = evaluate(args, test_dataset, model)\n",
    "                    all_eval_results.append(results)\n",
    "                    for key, value in results.items():\n",
    "                        tb_writer.add_scalar(\n",
    "                            'eval_{}'.format(key), value, global_step)\n",
    "                    tb_writer.add_scalar('eval_loss', eval_loss, global_step)\n",
    "                    # tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\n",
    "                        'train_loss', (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                # Save model checkpoint\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            epoch_iterator.close()\n",
    "            break\n",
    "\n",
    "    tb_writer.close()\n",
    "    return global_step, tr_loss/global_step, all_eval_results\n",
    "\n",
    "\n",
    "def evaluate(args, eval_dataset, model):\n",
    "    results = {}\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    collate_fn = get_collate_fn(args)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,\n",
    "                                 batch_size=args.eval_batch_size,\n",
    "                                 collate_fn=collate_fn)\n",
    "\n",
    "    # Eval\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for batch in eval_dataloader:\n",
    "    # for batch in tqdm(eval_dataloader, desc='Evaluating'):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = get_input_from_batch(args, batch)\n",
    "\n",
    "            logits = model(**inputs)\n",
    "            tmp_eval_loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = labels.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(\n",
    "                out_label_ids, labels.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    # print(preds)\n",
    "    result = compute_metrics(preds, out_label_ids)\n",
    "    results.update(result)\n",
    "\n",
    "    output_eval_file = os.path.join(args.output_dir, 'eval_results.txt')\n",
    "    with open(output_eval_file, 'a+') as writer:\n",
    "        logger.info('***** Eval results *****')\n",
    "        logger.info(\"  eval loss: %s\", str(eval_loss))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"  %s = %s\\n\" % (key, str(result[key])))\n",
    "            writer.write('\\n')\n",
    "        writer.write('\\n')\n",
    "    return results, eval_loss\n",
    "\n",
    "\n",
    "def evaluate_badcase(args, eval_dataset, model, word_vocab):\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    collate_fn = get_collate_fn(args)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,\n",
    "                                 batch_size=1,\n",
    "                                 collate_fn=collate_fn)\n",
    "\n",
    "    # Eval\n",
    "    badcases = []\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for batch in eval_dataloader:\n",
    "    # for batch in tqdm(eval_dataloader, desc='Evaluating'):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = get_input_from_batch(args, batch)\n",
    "\n",
    "            logits = model(**inputs)\n",
    "\n",
    "        pred = int(np.argmax(logits.detach().cpu().numpy(), axis=1)[0])\n",
    "        label = int(labels.detach().cpu().numpy()[0])\n",
    "        if pred != label:\n",
    "            sent_ids = inputs['input_ids'][0].detach().cpu().numpy()\n",
    "            aspect_ids = inputs['input_aspect_ids'][0].detach().cpu().numpy()\n",
    "            case = {}\n",
    "            case['sentence'] = args.tokenizer.decode(sent_ids)\n",
    "            case['aspect'] = args.tokenizer.decode(aspect_ids)\n",
    "            case['pred'] = pred\n",
    "            case['label'] = label\n",
    "            badcases.append(case)\n",
    "\n",
    "\n",
    "    return badcases\n",
    "\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    return acc_and_f1(preds, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = Parameters()\n",
    "# Required parameters\n",
    "parameters.dataset_name = \"rest\"\n",
    "parameters.output_dir = '/data1/SHENWZH/ABSA_online/data/output-gcn'\n",
    "parameters.num_classes = 3\n",
    "parameters.seed = 2019\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "parameters.glove_dir = \"glove\"\n",
    "parameters.num_layers = 2\n",
    "parameters.add_non_connect = True\n",
    "parameters.multi_hop = True\n",
    "parameters.max_hop = 4\n",
    "parameters.num_heads = 7\n",
    "parameters.dropout = 0.8\n",
    "parameters.num_gcn_layers = 1\n",
    "parameters.gcn_mem_dim = 300\n",
    "parameters.gcn_dropout = 0.2\n",
    "parameters.highway = 'store_true'\n",
    "parameters.bert_model_dir ='bert-base-uncased'\n",
    "parameters.tokenizer = BertTokenizer.from_pretrained(parameters.bert_model_dir)\n",
    "\n",
    "\n",
    "# GAT\n",
    "parameters.gat_attention_type = 'dotprod'\n",
    "parameters.embedding_type = 'bert'\n",
    "parameters.embedding_dim = 300\n",
    "parameters.dep_relation_embed_dim = 300    \n",
    "parameters.hidden_size = 300 \n",
    "parameters.final_hidden_size = 300 \n",
    "parameters.num_mlps = 2\n",
    "\n",
    "# Training parameters\n",
    "parameters.per_gpu_train_batch_size = 16\n",
    "parameters.per_gpu_eval_batch_size = 32\n",
    "parameters.gradient_accumulation_steps = 2\n",
    "parameters.learning_rate = 1e-3\n",
    "parameters.weight_decay = 0.0\n",
    "parameters.adam_epsilon = 1e-8\n",
    "parameters.max_grad_norm = 1.0\n",
    "parameters.num_train_epochs = 30.0\n",
    "parameters.max_steps = -1\n",
    "parameters.logging_steps = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n",
      "{'itos': ['<pad>', '<unk>', 'non-connect', 'ncon_3', 'ncon_2', 'ncon_4', 'det', 'nsubj', 'amod', 'conj', 'pobj', 'dobj', 'cc', 'prep', 'dep', 'poss', 'nn', 'advmod', 'rcmod', 'appos', 'cop', 'nsubjpass', 'num', 'partmod', 'aux', 'ccomp', 'advcl', 'predet', 'auxpass', 'infmod', 'xcomp', 'neg', 'mark', 'possessive', 'pcomp', 'iobj', 'parataxis', 'csubj', 'npadvmod', 'tmod', 'preconj', 'prt', 'expl', 'acomp', 'discourse', 'mwe'], 'stoi': defaultdict(<function _default_unk_index at 0x000001C18119F700>, {'<pad>': 0, '<unk>': 1, 'non-connect': 2, 'ncon_3': 3, 'ncon_2': 4, 'ncon_4': 5, 'det': 6, 'nsubj': 7, 'amod': 8, 'conj': 9, 'pobj': 10, 'dobj': 11, 'cc': 12, 'prep': 13, 'dep': 14, 'poss': 15, 'nn': 16, 'advmod': 17, 'rcmod': 18, 'appos': 19, 'cop': 20, 'nsubjpass': 21, 'num': 22, 'partmod': 23, 'aux': 24, 'ccomp': 25, 'advcl': 26, 'predet': 27, 'auxpass': 28, 'infmod': 29, 'xcomp': 30, 'neg': 31, 'mark': 32, 'possessive': 33, 'pcomp': 34, 'iobj': 35, 'parataxis': 36, 'csubj': 37, 'npadvmod': 38, 'tmod': 39, 'preconj': 40, 'prt': 41, 'expl': 42, 'acomp': 43, 'discourse': 44, 'mwe': 45, 'root': 1, 'punct': 1, 'number': 1, 'csubjpass': 1, 'quantmod': 1}), 'len': 46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "D:\\conda\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|                                                                                    | 0/30 [00:00<?, ?it/s]C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "Epoch:   3%|â–ˆâ–ˆâ–Œ                                                                         | 1/30 [00:36<17:27, 36.12s/it]C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "Epoch:   7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                       | 2/30 [01:12<17:02, 36.51s/it]C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "Epoch:   7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                       | 2/30 [01:36<22:36, 48.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0bb533018f57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mall_eval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_eval_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-3ed5845a3351>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args, train_dataset, model, test_dataset)\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# Log metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging_steps\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m                     \u001b[0mall_eval_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-3ed5845a3351>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(args, eval_dataset, model)\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_from_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m             \u001b[0mtmp_eval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-c2c081a33a3a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, input_aspect_ids, word_indexer, aspect_indexer, input_cat_ids, segment_ids, pos_class, dep_tags, text_len, aspect_len, dep_rels, dep_heads, aspect_position, dep_dirs)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mfmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_indexer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mword_indexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (Nï¼ŒL)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mfmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_cat_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mfeature_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# (N, L, D)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mpool_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#(N, D)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m         )\n\u001b[1;32m-> 1018\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m   1019\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    605\u001b[0m                 )\n\u001b[0;32m    606\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    608\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[0;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         )\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\transformers\\pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup CUDA, GPU training\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "parameters.device = device\n",
    "print('Device is {}'.format(parameters.device))\n",
    "\n",
    "# Load datasets and vocabs\n",
    "train_dataset, test_dataset, word_vocab, dep_tag_vocab, pos_tag_vocab= load_datasets_and_vocabs(parameters)\n",
    "\n",
    "print(dep_tag_vocab)\n",
    "\n",
    "# Build Model\n",
    "# model = Aspect_Text_Multi_Syntax_Encoding(args, dep_tag_vocab['len'], pos_tag_vocab['len'])\n",
    "\n",
    "model = Aspect_Bert_GAT(parameters, dep_tag_vocab['len'], pos_tag_vocab['len'])  # R-GAT + Bert\n",
    "\n",
    "model.to(parameters.device)\n",
    "# Train\n",
    "_, _,  all_eval_results = train(parameters, train_dataset, model, test_dataset)\n",
    "\n",
    "if len(all_eval_results):\n",
    "    best_eval_result = max(all_eval_results, key=lambda x: x['acc']) \n",
    "    print(best_eval_result)\n",
    "    for key in sorted(best_eval_result.keys()):\n",
    "        print(\"  %s = %s\", key, str(best_eval_result[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "a04f5d07b0747026a8fbcdf50b9443318e69b1b8bd6247d88bfadb4789282972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
