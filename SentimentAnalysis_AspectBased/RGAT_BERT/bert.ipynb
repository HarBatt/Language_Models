{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from model_gcn import GAT, GCN, Rel_GAT\n",
    "from model_utils import LinearAttention, DotprodAttention, RelationAttention, Highway, mask_logits\n",
    "from tree import *\n",
    "from datasets import load_datasets_and_vocabs\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AdamW\n",
    "from datasets import my_collate, my_collate_elmo, my_collate_pure_bert, my_collate_bert\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertConfig, BertPreTrainedModel, BertTokenizer\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from model_gcn import GAT, GCN, Rel_GAT\n",
    "from model_utils import LinearAttention, DotprodAttention, RelationAttention, Highway, mask_logits\n",
    "from tree import *\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import argparse\n",
    "import codecs\n",
    "import json\n",
    "import linecache\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import simplejson as json\n",
    "import torch\n",
    "from allennlp.modules.elmo import batch_to_ids\n",
    "from lxml import etree\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_and_vocabs(args):\n",
    "    train, test = get_dataset(args.dataset_name)\n",
    "\n",
    "    # Our model takes unrolled data, currently we don't consider the MAMS cases(future experiments)\n",
    "    _, train_all_unrolled, _, _ = get_rolled_and_unrolled_data(train, args)\n",
    "    _, test_all_unrolled, _, _ = get_rolled_and_unrolled_data(test, args)\n",
    "\n",
    "    logger.info('****** After unrolling ******')\n",
    "    logger.info('Train set size: %s', len(train_all_unrolled))\n",
    "    logger.info('Test set size: %s,', len(test_all_unrolled))\n",
    "\n",
    "    # Build word vocabulary(part of speech, dep_tag) and save pickles.\n",
    "    word_vecs, word_vocab, dep_tag_vocab, pos_tag_vocab = load_and_cache_vocabs(\n",
    "        train_all_unrolled+test_all_unrolled, args)\n",
    "\n",
    "    train_dataset = ASBA_Depparsed_Dataset(\n",
    "        train_all_unrolled, args, word_vocab, dep_tag_vocab, pos_tag_vocab)\n",
    "    test_dataset = ASBA_Depparsed_Dataset(\n",
    "        test_all_unrolled, args, word_vocab, dep_tag_vocab, pos_tag_vocab)\n",
    "\n",
    "    return train_dataset, test_dataset, word_vocab, dep_tag_vocab, pos_tag_vocab\n",
    "\n",
    "\n",
    "def read_sentence_depparsed(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        return data\n",
    "\n",
    "\n",
    "def get_dataset(dataset_name):\n",
    "    '''\n",
    "    Already preprocess the data and now they are in json format.(only for semeval14)\n",
    "    Retrieve train and test set\n",
    "    With a list of dict:\n",
    "    e.g. {\"sentence\": \"Boot time is super fast, around anywhere from 35 seconds to 1 minute.\",\n",
    "    \"tokens\": [\"Boot\", \"time\", \"is\", \"super\", \"fast\", \",\", \"around\", \"anywhere\", \"from\", \"35\", \"seconds\", \"to\", \"1\", \"minute\", \".\"],\n",
    "    \"tags\": [\"NNP\", \"NN\", \"VBZ\", \"RB\", \"RB\", \",\", \"RB\", \"RB\", \"IN\", \"CD\", \"NNS\", \"IN\", \"CD\", \"NN\", \".\"],\n",
    "    \"predicted_dependencies\": [\"nn\", \"nsubj\", \"root\", \"advmod\", \"advmod\", \"punct\", \"advmod\", \"advmod\", \"prep\", \"num\", \"pobj\", \"prep\", \"num\", \"pobj\", \"punct\"],\n",
    "    \"predicted_heads\": [2, 3, 0, 5, 3, 5, 8, 5, 8, 11, 9, 9, 14, 12, 3],\n",
    "    \"dependencies\": [[\"nn\", 2, 1], [\"nsubj\", 3, 2], [\"root\", 0, 3], [\"advmod\", 5, 4], [\"advmod\", 3, 5], [\"punct\", 5, 6], [\"advmod\", 8, 7], [\"advmod\", 5, 8],\n",
    "                    [\"prep\", 8, 9], [\"num\", 11, 10], [\"pobj\", 9, 11], [\"prep\", 9, 12], [\"num\", 14, 13], [\"pobj\", 12, 14], [\"punct\", 3, 15]],\n",
    "    \"aspect_sentiment\": [[\"Boot time\", \"positive\"]], \"from_to\": [[0, 2]]}\n",
    "    '''\n",
    "    rest_train = 'data/semeval14/Restaurants_Train_v2_biaffine_depparsed_with_energy.json'\n",
    "    rest_test = 'data/semeval14/Restaurants_Test_Gold_biaffine_depparsed_with_energy.json'\n",
    "\n",
    "    laptop_train = 'data/semeval14/Laptop_Train_v2_biaffine_depparsed.json'\n",
    "    laptop_test = 'data/semeval14/Laptops_Test_Gold_biaffine_depparsed.json'\n",
    "\n",
    "    twitter_train = 'data/twitter/train_biaffine.json'\n",
    "    twitter_test = 'data/twitter/test_biaffine.json'\n",
    "\n",
    "    ds_train = {'rest': rest_train,\n",
    "                'laptop': laptop_train, 'twitter': twitter_train}\n",
    "    ds_test = {'rest': rest_test,\n",
    "               'laptop': laptop_test, 'twitter': twitter_test}\n",
    "\n",
    "    train = list(read_sentence_depparsed(ds_train[dataset_name]))\n",
    "    logger.info('# Read %s Train set: %d', dataset_name, len(train))\n",
    "\n",
    "    test = list(read_sentence_depparsed(ds_test[dataset_name]))\n",
    "    logger.info(\"# Read %s Test set: %d\", dataset_name, len(test))\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def reshape_dependency_tree_new(as_start, as_end, dependencies, multi_hop=False, add_non_connect=False, tokens=None, max_hop = 5):\n",
    "    '''\n",
    "    Adding multi hops\n",
    "    This function is at the core of our algo, it reshape the dependency tree and center on the aspect.\n",
    "    In open-sourced edition, I choose not to take energy(the soft prediction of dependency from parser)\n",
    "    into consideration. For it requires tweaking allennlp's source code, and the energy is space-consuming.\n",
    "    And there are no significant difference in performance between the soft and the hard(with non-connect) version.\n",
    "\n",
    "    '''\n",
    "    dep_tag = []\n",
    "    dep_idx = []\n",
    "    dep_dir = []\n",
    "    # 1 hop\n",
    "\n",
    "    for i in range(as_start, as_end):\n",
    "        for dep in dependencies:\n",
    "            if i == dep[1] - 1:\n",
    "                # not root, not aspect\n",
    "                if (dep[2] - 1 < as_start or dep[2] - 1 >= as_end) and dep[2] != 0 and dep[2] - 1 not in dep_idx:\n",
    "                    if str(dep[0]) != 'punct':  # and tokens[dep[2] - 1] not in stopWords\n",
    "                        dep_tag.append(dep[0])\n",
    "                        dep_dir.append(1)\n",
    "                    else:\n",
    "                        dep_tag.append('<pad>')\n",
    "                        dep_dir.append(0)\n",
    "                    dep_idx.append(dep[2] - 1)\n",
    "            elif i == dep[2] - 1:\n",
    "                # not root, not aspect\n",
    "                if (dep[1] - 1 < as_start or dep[1] - 1 >= as_end) and dep[1] != 0 and dep[1] - 1 not in dep_idx:\n",
    "                    if str(dep[0]) != 'punct':  # and tokens[dep[1] - 1] not in stopWords\n",
    "                        dep_tag.append(dep[0])\n",
    "                        dep_dir.append(2)\n",
    "                    else:\n",
    "                        dep_tag.append('<pad>')\n",
    "                        dep_dir.append(0)\n",
    "                    dep_idx.append(dep[1] - 1)\n",
    "\n",
    "    if multi_hop:\n",
    "        current_hop = 2\n",
    "        added = True\n",
    "        while current_hop <= max_hop and len(dep_idx) < len(tokens) and added:\n",
    "            added = False\n",
    "            dep_idx_temp = deepcopy(dep_idx)\n",
    "            for i in dep_idx_temp:\n",
    "                for dep in dependencies:\n",
    "                    if i == dep[1] - 1:\n",
    "                        # not root, not aspect\n",
    "                        if (dep[2] - 1 < as_start or dep[2] - 1 >= as_end) and dep[2] != 0 and dep[2] - 1 not in dep_idx:\n",
    "                            if str(dep[0]) != 'punct':  # and tokens[dep[2] - 1] not in stopWords\n",
    "                                dep_tag.append('ncon_'+str(current_hop))\n",
    "                                dep_dir.append(1)\n",
    "                            else:\n",
    "                                dep_tag.append('<pad>')\n",
    "                                dep_dir.append(0)\n",
    "                            dep_idx.append(dep[2] - 1)\n",
    "                            added = True\n",
    "                    elif i == dep[2] - 1:\n",
    "                        # not root, not aspect\n",
    "                        if (dep[1] - 1 < as_start or dep[1] - 1 >= as_end) and dep[1] != 0 and dep[1] - 1 not in dep_idx:\n",
    "                            if str(dep[0]) != 'punct':  # and tokens[dep[1] - 1] not in stopWords\n",
    "                                dep_tag.append('ncon_'+str(current_hop))\n",
    "                                dep_dir.append(2)\n",
    "                            else:\n",
    "                                dep_tag.append('<pad>')\n",
    "                                dep_dir.append(0)\n",
    "                            dep_idx.append(dep[1] - 1)\n",
    "                            added = True\n",
    "            current_hop += 1\n",
    "\n",
    "    if add_non_connect:\n",
    "        for idx, token in enumerate(tokens):\n",
    "            if idx not in dep_idx and (idx < as_start or idx >= as_end):\n",
    "                dep_tag.append('non-connect')\n",
    "                dep_dir.append(0)\n",
    "                dep_idx.append(idx)\n",
    "\n",
    "    # add aspect and index, to make sure length matches len(tokens)\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if idx not in dep_idx:\n",
    "            dep_tag.append('<pad>')\n",
    "            dep_dir.append(0)\n",
    "            dep_idx.append(idx)\n",
    "\n",
    "    index = [i[0] for i in sorted(enumerate(dep_idx), key=lambda x:x[1])]\n",
    "    dep_tag = [dep_tag[i] for i in index]\n",
    "    dep_idx = [dep_idx[i] for i in index]\n",
    "    dep_dir = [dep_dir[i] for i in index]\n",
    "\n",
    "    assert len(tokens) == len(dep_idx), 'length wrong'\n",
    "    return dep_tag, dep_idx, dep_dir\n",
    "\n",
    "\n",
    "def reshape_dependency_tree(as_start, as_end, dependencies, add_2hop=False, add_non_connect=False, tokens=None):\n",
    "    '''\n",
    "    This function is at the core of our algo, it reshape the dependency tree and center on the aspect.\n",
    "\n",
    "    In open-sourced edition, I choose not to take energy(the soft prediction of dependency from parser)\n",
    "    into consideration. For it requires tweaking allennlp's source code, and the energy is space-consuming.\n",
    "    And there are no significant difference in performance between the soft and the hard(with non-connect) version.\n",
    "\n",
    "    '''\n",
    "    dep_tag = []\n",
    "    dep_idx = []\n",
    "    dep_dir = []\n",
    "    # 1 hop\n",
    "\n",
    "    for i in range(as_start, as_end):\n",
    "        for dep in dependencies:\n",
    "            if i == dep[1] - 1:\n",
    "                # not root, not aspect\n",
    "                if (dep[2] - 1 < as_start or dep[2] - 1 >= as_end) and dep[2] != 0 and dep[2] - 1 not in dep_idx:\n",
    "                    if str(dep[0]) != 'punct':  # and tokens[dep[2] - 1] not in stopWords\n",
    "                        dep_tag.append(dep[0])\n",
    "                        dep_dir.append(1)\n",
    "                    else:\n",
    "                        dep_tag.append('<pad>')\n",
    "                        dep_dir.append(0)\n",
    "                    dep_idx.append(dep[2] - 1)\n",
    "            elif i == dep[2] - 1:\n",
    "                # not root, not aspect\n",
    "                if (dep[1] - 1 < as_start or dep[1] - 1 >= as_end) and dep[1] != 0 and dep[1] - 1 not in dep_idx:\n",
    "                    if str(dep[0]) != 'punct':  # and tokens[dep[1] - 1] not in stopWords\n",
    "                        dep_tag.append(dep[0])\n",
    "                        dep_dir.append(2)\n",
    "                    else:\n",
    "                        dep_tag.append('<pad>')\n",
    "                        dep_dir.append(0)\n",
    "                    dep_idx.append(dep[1] - 1)\n",
    "\n",
    "    # 2 hop\n",
    "    if add_2hop:\n",
    "        dep_idx_cp = dep_idx\n",
    "        for i in dep_idx_cp:\n",
    "            for dep in dependencies:\n",
    "                # connect to i, not a punct\n",
    "                if i == dep[1] - 1 and str(dep[0]) != 'punct':\n",
    "                    # not root, not aspect\n",
    "                    if (dep[2] - 1 < as_start or dep[2] - 1 >= as_end) and dep[2] != 0:\n",
    "                        if dep[2]-1 not in dep_idx:\n",
    "                            dep_tag.append(dep[0])\n",
    "                            dep_idx.append(dep[2] - 1)\n",
    "                # connect to i, not a punct\n",
    "                elif i == dep[2] - 1 and str(dep[0]) != 'punct':\n",
    "                    # not root, not aspect\n",
    "                    if (dep[1] - 1 < as_start or dep[1] - 1 >= as_end) and dep[1] != 0:\n",
    "                        if dep[1]-1 not in dep_idx:\n",
    "                            dep_tag.append(dep[0])\n",
    "                            dep_idx.append(dep[1] - 1)\n",
    "    if add_non_connect:\n",
    "        for idx, token in enumerate(tokens):\n",
    "            if idx not in dep_idx and (idx < as_start or idx >= as_end):\n",
    "                dep_tag.append('non-connect')\n",
    "                dep_dir.append(0)\n",
    "                dep_idx.append(idx)\n",
    "\n",
    "    # add aspect and index, to make sure length matches len(tokens)\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if idx not in dep_idx:\n",
    "            dep_tag.append('<pad>')\n",
    "            dep_dir.append(0)\n",
    "            dep_idx.append(idx)\n",
    "\n",
    "    index = [i[0] for i in sorted(enumerate(dep_idx), key=lambda x:x[1])]\n",
    "    dep_tag = [dep_tag[i] for i in index]\n",
    "    dep_idx = [dep_idx[i] for i in index]\n",
    "    dep_dir = [dep_dir[i] for i in index]\n",
    "\n",
    "    assert len(tokens) == len(dep_idx), 'length wrong'\n",
    "    return dep_tag, dep_idx, dep_dir\n",
    "\n",
    "def get_rolled_and_unrolled_data(input_data, args):\n",
    "    '''\n",
    "    In input_data, each sentence could have multiple aspects with different sentiments.\n",
    "    Our method treats each sentence with one aspect at a time, so even for\n",
    "    multi-aspect-multi-sentiment sentences, we will unroll them to single aspect sentence.\n",
    "\n",
    "    Perform reshape_dependency_tree to each sentence with aspect\n",
    "\n",
    "    return:\n",
    "        all_rolled:\n",
    "                a list of dict\n",
    "                    {sentence, tokens, pos_tags, pos_class, aspects(list of aspects), sentiments(list of sentiments)\n",
    "                    froms, tos, dep_tags, dep_index, dependencies}\n",
    "        all_unrolled:\n",
    "                unrolled, with aspect(single), sentiment(single) and so on...\n",
    "        mixed_rolled:\n",
    "                Multiple aspects and multiple sentiments, ROLLED.\n",
    "        mixed_unrolled:\n",
    "                Multiple aspects and multiple sentiments, UNROLLED.\n",
    "    '''\n",
    "    # A hand-picked set of part of speech tags that we see contributes to ABSA.\n",
    "    opinionated_tags = ['JJ', 'JJR', 'JJS', 'RB', 'RBR',\n",
    "                        'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "    all_rolled = []\n",
    "    all_unrolled = []\n",
    "    mixed_rolled = []\n",
    "    mixed_unrolled = []\n",
    "\n",
    "    unrolled = []\n",
    "    mixed = []\n",
    "    unrolled_ours = []\n",
    "    mixed_ours = []\n",
    "\n",
    "    # Make sure the tree is successfully built.\n",
    "    zero_dep_counter = 0\n",
    "\n",
    "    # Sentiment counters\n",
    "    total_counter = defaultdict(int)\n",
    "    mixed_counter = defaultdict(int)\n",
    "    sentiments_lookup = {'negative': 0, 'positive': 1, 'neutral': 2}\n",
    "\n",
    "    logger.info('*** Start processing data(unrolling and reshaping) ***')\n",
    "    tree_samples = []\n",
    "    # for seeking 'but' examples\n",
    "    for e in input_data:\n",
    "        e['tokens'] = [x.lower() for x in e['tokens']]\n",
    "        aspects = []\n",
    "        sentiments = []\n",
    "        froms = []\n",
    "        tos = []\n",
    "        dep_tags = []\n",
    "        dep_index = []\n",
    "        dep_dirs = []\n",
    "\n",
    "        # Classify based on POS-tags\n",
    "\n",
    "        pos_class = e['tags']\n",
    "\n",
    "        # Iterate through aspects in a sentence and reshape the dependency tree.\n",
    "        for i in range(len(e['aspect_sentiment'])):\n",
    "            aspect = e['aspect_sentiment'][i][0].lower()\n",
    "            # We would tokenize the aspect while at it.\n",
    "            aspect = word_tokenize(aspect)\n",
    "            sentiment = sentiments_lookup[e['aspect_sentiment'][i][1]]\n",
    "            frm = e['from_to'][i][0]\n",
    "            to = e['from_to'][i][1]\n",
    "\n",
    "            aspects.append(aspect)\n",
    "            sentiments.append(sentiment)\n",
    "            froms.append(frm)\n",
    "            tos.append(to)\n",
    "\n",
    "            # Center on the aspect.\n",
    "            dep_tag, dep_idx, dep_dir = reshape_dependency_tree_new(frm, to, e['dependencies'],\n",
    "                                                       multi_hop=args.multi_hop, add_non_connect=args.add_non_connect, tokens=e['tokens'], max_hop=args.max_hop)\n",
    "\n",
    "            # Because of tokenizer differences, aspect opsitions are off, so we find the index and try again.\n",
    "            if len(dep_tag) == 0:\n",
    "                zero_dep_counter += 1\n",
    "                as_sent = e['aspect_sentiment'][i][0].split()\n",
    "                as_start = e['tokens'].index(as_sent[0])\n",
    "                # print(e['tokens'], e['aspect_sentiment'], e['dependencies'],as_sent[0])\n",
    "                as_end = e['tokens'].index(\n",
    "                    as_sent[-1]) if len(as_sent) > 1 else as_start + 1\n",
    "                print(\"Debugging: as_start as_end \", as_start, as_end)\n",
    "                dep_tag, dep_idx, dep_dir = reshape_dependency_tree_new(as_start, as_end, e['dependencies'],\n",
    "                                                           multi_hop=args.multi_hop, add_non_connect=args.add_non_connect, tokens=e['tokens'], max_hop=args.max_hop)\n",
    "                if len(dep_tag) == 0:  # for debugging\n",
    "                    print(\"Debugging: zero_dep\",\n",
    "                          e['aspect_sentiment'][i][0], e['tokens'])\n",
    "                    print(\"Debugging: \". e['dependencies'])\n",
    "                else:\n",
    "                    zero_dep_counter -= 1\n",
    "\n",
    "            dep_tags.append(dep_tag)\n",
    "            dep_index.append(dep_idx)\n",
    "            dep_dirs.append(dep_dir)\n",
    "\n",
    "            total_counter[e['aspect_sentiment'][i][1]] += 1\n",
    "\n",
    "            # Unrolling\n",
    "            all_unrolled.append(\n",
    "                {'sentence': e['tokens'], 'tags': e['tags'], 'pos_class': pos_class, 'aspect': aspect, 'sentiment': sentiment,\n",
    "                    'predicted_dependencies': e['predicted_dependencies'], 'predicted_heads': e['predicted_heads'],\n",
    "                 'from': frm, 'to': to, 'dep_tag': dep_tag, 'dep_idx': dep_idx, 'dep_dir':dep_dir,'dependencies': e['dependencies']})\n",
    "\n",
    "\n",
    "        # All sentences with multiple aspects and sentiments rolled.\n",
    "        all_rolled.append(\n",
    "            {'sentence': e['tokens'], 'tags': e['tags'], 'pos_class': pos_class, 'aspects': aspects, 'sentiments': sentiments,\n",
    "             'from': froms, 'to': tos, 'dep_tags': dep_tags, 'dep_index': dep_index, 'dependencies': e['dependencies']})\n",
    "\n",
    "        # Ignore sentences with single aspect or no aspect\n",
    "        if len(e['aspect_sentiment']) and len(set(map(lambda x: x[1], e['aspect_sentiment']))) > 1:\n",
    "            mixed_rolled.append(\n",
    "                {'sentence': e['tokens'], 'tags': e['tags'], 'pos_class': pos_class, 'aspects': aspects, 'sentiments': sentiments,\n",
    "                 'from': froms, 'to': tos, 'dep_tags': dep_tags, 'dep_index': dep_index, 'dependencies': e['dependencies']})\n",
    "\n",
    "            # Unrolling\n",
    "            for i, as_sent in enumerate(e['aspect_sentiment']):\n",
    "                mixed_counter[as_sent[1]] += 1\n",
    "                mixed_unrolled.append(\n",
    "                    {'sentence': e['tokens'], 'tags': e['tags'], 'pos_class': pos_class, 'aspect': aspects[i], 'sentiment': sentiments[i],\n",
    "                     'from': froms[i], 'to': tos[i], 'dep_tag': dep_tags[i], 'dep_idx': dep_index[i], 'dependencies': e['dependencies']})\n",
    "\n",
    "\n",
    "    logger.info('Total sentiment counter: %s', total_counter)\n",
    "    logger.info('Multi-Aspect-Multi-Sentiment counter: %s', mixed_counter)\n",
    "\n",
    "    return all_rolled, all_unrolled, mixed_rolled, mixed_unrolled\n",
    "\n",
    "\n",
    "def load_and_cache_vocabs(data, args):\n",
    "    '''\n",
    "    Build vocabulary of words, part of speech tags, dependency tags and cache them.\n",
    "    Load glove embedding if needed.\n",
    "    '''\n",
    "    pkls_path = os.path.join(args.output_dir, 'pkls')\n",
    "    if not os.path.exists(pkls_path):\n",
    "        os.makedirs(pkls_path)\n",
    "\n",
    "    # Build or load word vocab and glove embeddings.\n",
    "    # Elmo and bert have it's own vocab and embeddings.\n",
    "    if args.embedding_type == 'glove':\n",
    "        cached_word_vocab_file = os.path.join(\n",
    "            pkls_path, 'cached_{}_{}_word_vocab.pkl'.format(args.dataset_name, args.embedding_type))\n",
    "        if os.path.exists(cached_word_vocab_file):\n",
    "            logger.info('Loading word vocab from %s', cached_word_vocab_file)\n",
    "            with open(cached_word_vocab_file, 'rb') as f:\n",
    "                word_vocab = pickle.load(f)\n",
    "        else:\n",
    "            logger.info('Creating word vocab from dataset %s',\n",
    "                        args.dataset_name)\n",
    "            word_vocab = build_text_vocab(data)\n",
    "            logger.info('Word vocab size: %s', word_vocab['len'])\n",
    "            logging.info('Saving word vocab to %s', cached_word_vocab_file)\n",
    "            with open(cached_word_vocab_file, 'wb') as f:\n",
    "                pickle.dump(word_vocab, f, -1)\n",
    "\n",
    "        cached_word_vecs_file = os.path.join(pkls_path, 'cached_{}_{}_word_vecs.pkl'.format(\n",
    "            args.dataset_name, args.embedding_type))\n",
    "        if os.path.exists(cached_word_vecs_file):\n",
    "            logger.info('Loading word vecs from %s', cached_word_vecs_file)\n",
    "            with open(cached_word_vecs_file, 'rb') as f:\n",
    "                word_vecs = pickle.load(f)\n",
    "        else:\n",
    "            logger.info('Creating word vecs from %s', args.glove_dir)\n",
    "            word_vecs = load_glove_embedding(\n",
    "                word_vocab['itos'], args.glove_dir, 0.25, args.embedding_dim)\n",
    "            logger.info('Saving word vecs to %s', cached_word_vecs_file)\n",
    "            with open(cached_word_vecs_file, 'wb') as f:\n",
    "                pickle.dump(word_vecs, f, -1)\n",
    "    else:\n",
    "        word_vocab = None\n",
    "        word_vecs = None\n",
    "\n",
    "    # Build vocab of dependency tags\n",
    "    cached_dep_tag_vocab_file = os.path.join(\n",
    "        pkls_path, 'cached_{}_dep_tag_vocab.pkl'.format(args.dataset_name))\n",
    "    if os.path.exists(cached_dep_tag_vocab_file):\n",
    "        logger.info('Loading vocab of dependency tags from %s',\n",
    "                    cached_dep_tag_vocab_file)\n",
    "        with open(cached_dep_tag_vocab_file, 'rb') as f:\n",
    "            dep_tag_vocab = pickle.load(f)\n",
    "    else:\n",
    "        logger.info('Creating vocab of dependency tags.')\n",
    "        dep_tag_vocab = build_dep_tag_vocab(data, min_freq=0)\n",
    "        logger.info('Saving dependency tags  vocab, size: %s, to file %s',\n",
    "                    dep_tag_vocab['len'], cached_dep_tag_vocab_file)\n",
    "        with open(cached_dep_tag_vocab_file, 'wb') as f:\n",
    "            pickle.dump(dep_tag_vocab, f, -1)\n",
    "\n",
    "    # Build vocab of part of speech tags.\n",
    "    cached_pos_tag_vocab_file = os.path.join(\n",
    "        pkls_path, 'cached_{}_pos_tag_vocab.pkl'.format(args.dataset_name))\n",
    "    if os.path.exists(cached_pos_tag_vocab_file):\n",
    "        logger.info('Loading vocab of dependency tags from %s',\n",
    "                    cached_pos_tag_vocab_file)\n",
    "        with open(cached_pos_tag_vocab_file, 'rb') as f:\n",
    "            pos_tag_vocab = pickle.load(f)\n",
    "    else:\n",
    "        logger.info('Creating vocab of dependency tags.')\n",
    "        pos_tag_vocab = build_pos_tag_vocab(data, min_freq=0)\n",
    "        logger.info('Saving dependency tags  vocab, size: %s, to file %s',\n",
    "                    pos_tag_vocab['len'], cached_pos_tag_vocab_file)\n",
    "        with open(cached_pos_tag_vocab_file, 'wb') as f:\n",
    "            pickle.dump(pos_tag_vocab, f, -1)\n",
    "\n",
    "    return word_vecs, word_vocab, dep_tag_vocab, pos_tag_vocab\n",
    "\n",
    "\n",
    "\n",
    "def _default_unk_index():\n",
    "    return 1\n",
    "\n",
    "\n",
    "def build_text_vocab(data, vocab_size=100000, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for d in data:\n",
    "        s = d['sentence']\n",
    "        counter.update(s)\n",
    "\n",
    "    itos = ['[PAD]', '[UNK]']\n",
    "    min_freq = max(min_freq, 1)\n",
    "\n",
    "    # sort by frequency, then alphabetically\n",
    "    words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "    words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "    for word, freq in words_and_frequencies:\n",
    "        if freq < min_freq or len(itos) == vocab_size:\n",
    "            break\n",
    "        itos.append(word)\n",
    "    # stoi is simply a reverse dict for itos\n",
    "    stoi = defaultdict(_default_unk_index)\n",
    "    stoi.update({tok: i for i, tok in enumerate(itos)})\n",
    "\n",
    "    return {'itos': itos, 'stoi': stoi, 'len': len(itos)}\n",
    "\n",
    "\n",
    "def build_pos_tag_vocab(data, vocab_size=1000, min_freq=1):\n",
    "    \"\"\"\n",
    "    Part of speech tags vocab.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for d in data:\n",
    "        tags = d['tags']\n",
    "        counter.update(tags)\n",
    "\n",
    "    itos = ['<pad>']\n",
    "    min_freq = max(min_freq, 1)\n",
    "\n",
    "    # sort by frequency, then alphabetically\n",
    "    words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "    words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "    for word, freq in words_and_frequencies:\n",
    "        if freq < min_freq or len(itos) == vocab_size:\n",
    "            break\n",
    "        itos.append(word)\n",
    "    # stoi is simply a reverse dict for itos\n",
    "    stoi = defaultdict()\n",
    "    stoi.update({tok: i for i, tok in enumerate(itos)})\n",
    "\n",
    "    return {'itos': itos, 'stoi': stoi, 'len': len(itos)}\n",
    "\n",
    "\n",
    "# def build_dep_tag_vocab_energy():  # 47 in total, all tags plus pad and non-connect\n",
    "#     '''\n",
    "#     biaffine dep_tag Vocab : {0: 'punct', 1: 'prep', 2: 'pobj', 3: 'det', 4: 'nn',\n",
    "#         5: 'nsubj', 6: 'amod', 7: 'root', 8: 'dobj', 9: 'aux', 10: 'advmod', 11: 'conj',\n",
    "#         12: 'cc', 13: 'num', 14: 'poss', 15: 'ccomp', 16: 'dep', 17: 'xcomp', 18: 'mark',\n",
    "#         19: 'cop', 20: 'number', 21: 'possessive', 22: 'rcmod', 23: 'auxpass', 24: 'appos',\n",
    "#         25: 'nsubjpass', 26: 'advcl', 27: 'partmod', 28: 'pcomp', 29: 'neg', 30: 'tmod',\n",
    "#         31: 'quantmod', 32: 'npadvmod', 33: 'prt', 34: 'infmod', 35: 'parataxis',\n",
    "#         36: 'mwe', 37: 'expl', 38: 'acomp', 39: 'iobj', 40: 'csubj', 41: 'predet',\n",
    "#         42: 'preconj', 43: 'discourse', 44: 'csubjpass'}\n",
    "#     This is used in energy case.\n",
    "#     '''\n",
    "#     head_tags = {0: 'punct', 1: 'prep', 2: 'pobj', 3: 'det', 4: 'nn', 5: 'nsubj', 6: 'amod', 7: 'root', 8: 'dobj', 9: 'aux', 10: 'advmod', 11: 'conj', 12: 'cc', 13: 'num', 14: 'poss', 15: 'ccomp', 16: 'dep', 17: 'xcomp', 18: 'mark', 19: 'cop', 20: 'number', 21: 'possessive', 22: 'rcmod', 23: 'auxpass', 24: 'appos',\n",
    "#                  25: 'nsubjpass', 26: 'advcl', 27: 'partmod', 28: 'pcomp', 29: 'neg', 30: 'tmod', 31: 'quantmod', 32: 'npadvmod', 33: 'prt', 34: 'infmod', 35: 'parataxis', 36: 'mwe', 37: 'expl', 38: 'acomp', 39: 'iobj', 40: 'csubj', 41: 'predet', 42: 'preconj', 43: 'discourse', 44: 'csubjpass', 45: '<pad>', 46: 'non-connect'}\n",
    "#     itos = [head_tags[i] for i in range(len(head_tags))]\n",
    "#     stoi = defaultdict()\n",
    "#     stoi.update({tok: i for i, tok in enumerate(itos)})\n",
    "#     return {'itos': itos, 'stoi': stoi, 'len': len(itos)}\n",
    "\n",
    "\n",
    "def build_dep_tag_vocab(data, vocab_size=1000, min_freq=0):\n",
    "    counter = Counter()\n",
    "    for d in data:\n",
    "        tags = d['dep_tag']\n",
    "        counter.update(tags)\n",
    "\n",
    "    itos = ['<pad>', '<unk>']\n",
    "    min_freq = max(min_freq, 1)\n",
    "\n",
    "    # sort by frequency, then alphabetically\n",
    "    words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "    words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "    for word, freq in words_and_frequencies:\n",
    "        if freq < min_freq or len(itos) == vocab_size:\n",
    "            break\n",
    "        if word == '<pad>':\n",
    "            continue\n",
    "        itos.append(word)\n",
    "    # stoi is simply a reverse dict for itos\n",
    "    stoi = defaultdict(_default_unk_index)\n",
    "    stoi.update({tok: i for i, tok in enumerate(itos)})\n",
    "\n",
    "    return {'itos': itos, 'stoi': stoi, 'len': len(itos)}\n",
    "\n",
    "\n",
    "class ASBA_Depparsed_Dataset(Dataset):\n",
    "    '''\n",
    "    Convert examples to features, numericalize text to ids.\n",
    "    data:\n",
    "        -list of dict:\n",
    "            keys: sentence, tags, pos_class, aspect, sentiment,\n",
    "                predicted_dependencies, predicted_heads,\n",
    "                from, to, dep_tag, dep_idx, dependencies, dep_dir\n",
    "\n",
    "    After processing,\n",
    "    data:\n",
    "        sentence\n",
    "        tags\n",
    "        pos_class\n",
    "        aspect\n",
    "        sentiment\n",
    "        from\n",
    "        to\n",
    "        dep_tag\n",
    "        dep_idx\n",
    "        dep_dir\n",
    "        predicted_dependencies_ids\n",
    "        predicted_heads\n",
    "        dependencies\n",
    "        sentence_ids\n",
    "        aspect_ids\n",
    "        tag_ids\n",
    "        dep_tag_ids\n",
    "        text_len\n",
    "        aspect_len\n",
    "        if bert:\n",
    "            input_ids\n",
    "            word_indexer\n",
    "\n",
    "    Return from getitem:\n",
    "        sentence_ids\n",
    "        aspect_ids\n",
    "        dep_tag_ids\n",
    "        dep_dir_ids\n",
    "        pos_class\n",
    "        text_len\n",
    "        aspect_len\n",
    "        sentiment\n",
    "        deprel\n",
    "        dephead\n",
    "        aspect_position\n",
    "        if bert:\n",
    "            input_ids\n",
    "            word_indexer\n",
    "            input_aspect_ids\n",
    "            aspect_indexer\n",
    "        or:\n",
    "            input_cat_ids\n",
    "            segment_ids\n",
    "    '''\n",
    "\n",
    "    def __init__(self, data, args, word_vocab, dep_tag_vocab, pos_tag_vocab):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "        self.word_vocab = word_vocab\n",
    "        self.dep_tag_vocab = dep_tag_vocab\n",
    "        self.pos_tag_vocab = pos_tag_vocab\n",
    "\n",
    "        self.convert_features()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        items = e['dep_tag_ids'], \\\n",
    "            e['pos_class'], e['text_len'], e['aspect_len'], e['sentiment'],\\\n",
    "            e['dep_rel_ids'], e['predicted_heads'], e['aspect_position'], e['dep_dir_ids']\n",
    "\n",
    "        bert_items = e['input_cat_ids'], e['segment_ids']\n",
    "        items_tensor = tuple(torch.tensor(t) for t in bert_items)\n",
    "        items_tensor += tuple(torch.tensor(t) for t in items)\n",
    "\n",
    "        return items_tensor\n",
    "\n",
    "    def convert_features_bert(self, i):\n",
    "        \"\"\"\n",
    "        BERT features.\n",
    "        convert sentence to feature. \n",
    "        \"\"\"\n",
    "        cls_token = \"[CLS]\"\n",
    "        sep_token = \"[SEP]\"\n",
    "        pad_token = 0\n",
    "        # tokenizer = self.args.tokenizer\n",
    "\n",
    "        tokens = []\n",
    "        word_indexer = []\n",
    "        aspect_tokens = []\n",
    "        aspect_indexer = []\n",
    "\n",
    "        for word in self.data[i]['sentence']:\n",
    "            word_tokens = self.args.tokenizer.tokenize(word)\n",
    "            token_idx = len(tokens)\n",
    "            tokens.extend(word_tokens)\n",
    "            # word_indexer is for indexing after bert, feature back to the length of original length.\n",
    "            word_indexer.append(token_idx)\n",
    "\n",
    "        # aspect\n",
    "        for word in self.data[i]['aspect']:\n",
    "            word_aspect_tokens = self.args.tokenizer.tokenize(word)\n",
    "            token_idx = len(aspect_tokens)\n",
    "            aspect_tokens.extend(word_aspect_tokens)\n",
    "            aspect_indexer.append(token_idx)\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids:   0   0   0   0  0     0   0\n",
    "\n",
    "        tokens = [cls_token] + tokens + [sep_token]\n",
    "        aspect_tokens = [cls_token] + aspect_tokens + [sep_token]\n",
    "        word_indexer = [i+1 for i in word_indexer]\n",
    "        aspect_indexer = [i+1 for i in aspect_indexer]\n",
    "\n",
    "        input_ids = self.args.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_aspect_ids = self.args.tokenizer.convert_tokens_to_ids(\n",
    "            aspect_tokens)\n",
    "\n",
    "        # check len of word_indexer equals to len of sentence.\n",
    "        assert len(word_indexer) == len(self.data[i]['sentence'])\n",
    "        assert len(aspect_indexer) == len(self.data[i]['aspect'])\n",
    "\n",
    "        # THE STEP:Zero-pad up to the sequence length, save to collate_fn.\n",
    "\n",
    "        input_cat_ids = input_ids + input_aspect_ids[1:]\n",
    "        segment_ids = [0] * len(input_ids) + [1] * len(input_aspect_ids[1:])\n",
    "\n",
    "        self.data[i]['input_cat_ids'] = input_cat_ids\n",
    "        self.data[i]['segment_ids'] = segment_ids\n",
    "\n",
    "\n",
    "    def convert_features(self):\n",
    "        '''\n",
    "        Convert sentence, aspects, pos_tags, dependency_tags to ids.\n",
    "        '''\n",
    "        for i in range(len(self.data)):\n",
    "            self.convert_features_bert(i)\n",
    "\n",
    "            self.data[i]['text_len'] = len(self.data[i]['sentence'])\n",
    "            self.data[i]['aspect_position'] = [0] * self.data[i]['text_len']\n",
    "            try:  # find the index of aspect in sentence\n",
    "                for j in range(self.data[i]['from'], self.data[i]['to']):\n",
    "                    self.data[i]['aspect_position'][j] = 1\n",
    "            except:\n",
    "                for term in self.data[i]['aspect']:\n",
    "                    self.data[i]['aspect_position'][self.data[i]\n",
    "                                                    ['sentence'].index(term)] = 1\n",
    "\n",
    "            self.data[i]['dep_tag_ids'] = [self.dep_tag_vocab['stoi'][w]\n",
    "                                           for w in self.data[i]['dep_tag']]\n",
    "            self.data[i]['dep_dir_ids'] = [idx\n",
    "                                           for idx in self.data[i]['dep_dir']]\n",
    "            self.data[i]['pos_class'] = [self.pos_tag_vocab['stoi'][w]\n",
    "                                             for w in self.data[i]['tags']]\n",
    "            self.data[i]['aspect_len'] = len(self.data[i]['aspect'])\n",
    "\n",
    "            self.data[i]['dep_rel_ids'] = [self.dep_tag_vocab['stoi'][r]\n",
    "                                           for r in self.data[i]['predicted_dependencies']]\n",
    "\n",
    "\n",
    "def my_collate_pure_bert(batch):\n",
    "    '''\n",
    "    Pad sentence and aspect in a batch.\n",
    "    Sort the sentences based on length.\n",
    "    Turn all into tensors.\n",
    "\n",
    "    Process bert feature\n",
    "    Pure Bert: cat text and aspect, cls to predict.\n",
    "    Test indexing while at it?\n",
    "    '''\n",
    "    # sentence_ids, aspect_ids\n",
    "    input_cat_ids, segment_ids, dep_tag_ids, pos_class, text_len, aspect_len, sentiment, dep_rel_ids, dep_heads, aspect_positions, dep_dir_ids = zip(\n",
    "        *batch)  # from Dataset.__getitem__()\n",
    "\n",
    "    text_len = torch.tensor(text_len)\n",
    "    aspect_len = torch.tensor(aspect_len)\n",
    "    sentiment = torch.tensor(sentiment)\n",
    "\n",
    "    # Pad sequences.\n",
    "    input_cat_ids = pad_sequence(\n",
    "        input_cat_ids, batch_first=True, padding_value=0)\n",
    "    segment_ids = pad_sequence(segment_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "    aspect_positions = pad_sequence(\n",
    "        aspect_positions, batch_first=True, padding_value=0)\n",
    "\n",
    "    dep_tag_ids = pad_sequence(dep_tag_ids, batch_first=True, padding_value=0)\n",
    "    dep_dir_ids = pad_sequence(dep_dir_ids, batch_first=True, padding_value=0)\n",
    "    pos_class = pad_sequence(pos_class, batch_first=True, padding_value=0)\n",
    "\n",
    "    dep_rel_ids = pad_sequence(dep_rel_ids, batch_first=True, padding_value=0)\n",
    "    dep_heads = pad_sequence(dep_heads, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Sort all tensors based on text len.\n",
    "    _, sorted_idx = text_len.sort(descending=True)\n",
    "    input_cat_ids = input_cat_ids[sorted_idx]\n",
    "    segment_ids = segment_ids[sorted_idx]\n",
    "    aspect_positions = aspect_positions[sorted_idx]\n",
    "    dep_tag_ids = dep_tag_ids[sorted_idx]\n",
    "\n",
    "    dep_dir_ids = dep_dir_ids[sorted_idx]\n",
    "    pos_class = pos_class[sorted_idx]\n",
    "    text_len = text_len[sorted_idx]\n",
    "    aspect_len = aspect_len[sorted_idx]\n",
    "    sentiment = sentiment[sorted_idx]\n",
    "    dep_rel_ids = dep_rel_ids[sorted_idx]\n",
    "    dep_heads = dep_heads[sorted_idx]\n",
    "\n",
    "    return input_cat_ids, segment_ids, dep_tag_ids, pos_class, text_len, aspect_len, sentiment, dep_rel_ids, dep_heads, aspect_positions, dep_dir_ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pure_Bert(nn.Module):\n",
    "    '''\n",
    "    Bert for sequence classification.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, args, hidden_size=256):\n",
    "        super(Pure_Bert, self).__init__()\n",
    "\n",
    "        config = BertConfig.from_pretrained(args.bert_model_dir)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(args.bert_model_dir)\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            args.bert_model_dir, config=config)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        layers = [nn.Linear(\n",
    "            config.hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, args.num_classes)]\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "        print(\"Trainable Parameters {}\".format(get_parameters_count(self.bert)))\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        outputs = self.bert(input_ids, token_type_ids=token_type_ids)\n",
    "        # pool output is usually *not* a good summary of the semantic content of the input,\n",
    "        # you're often better with averaging or poolin the sequence of hidden-states for the whole input sequence.\n",
    "        pooled_output = outputs[1]\n",
    "        # pooled_output = torch.mean(pooled_output, dim = 1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def get_parameters_count(module):\n",
    "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "def get_input_from_batch(args, batch):\n",
    "    # input_cat_ids, segment_ids, dep_tag_ids, pos_class, text_len, aspect_len, sentiment, dep_rel_ids, dep_heads, aspect_positions\n",
    "    inputs = {  'input_ids': batch[0],\n",
    "                'token_type_ids': batch[1]}\n",
    "    labels = batch[6]\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def get_collate_fn(args):\n",
    "    return my_collate_pure_bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_optimizer(args, model):\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    # scheduler = WarmupLinearSchedule(\n",
    "    #     optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(args, train_dataset, model, test_dataset):\n",
    "    '''Train the model'''\n",
    "    tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    collate_fn = get_collate_fn(args)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler,\n",
    "                                  batch_size=args.train_batch_size,\n",
    "                                  collate_fn=collate_fn)\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (\n",
    "            len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(\n",
    "            train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    \n",
    "    \n",
    "    optimizer = get_bert_optimizer(args, model)\n",
    "\n",
    "\n",
    "    # Train\n",
    "    print(\"***** Running training *****\")\n",
    "    print(\"Num examples: {}; Epochs: {}; Gradient Accumulation steps: {}; Total optimization steps: {}\".format(len(train_dataset), args.num_train_epochs, args.gradient_accumulation_steps, t_total))\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    all_eval_results = []\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "    set_seed(args)\n",
    "    for _ in train_iterator:\n",
    "        # epoch_iterator = tqdm(train_dataloader, desc='Iteration')\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "            inputs, labels = get_input_from_batch(args, batch)\n",
    "            #print(inputs['sentence'])\n",
    "            \n",
    "            #print(inputs.shape, labels.shape, batch.shape)\n",
    "            logit = model(**inputs)\n",
    "            #print(\"Shape: {}, {}\".format(logit.shape, labels.shape))\n",
    "            loss = F.cross_entropy(logit, labels)\n",
    "\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                # scheduler.step()  # Update learning rate schedule\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                # Log metrics\n",
    "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    results, eval_loss = evaluate(args, test_dataset, model)\n",
    "                    all_eval_results.append(results)\n",
    "                    for key, value in results.items():\n",
    "                        tb_writer.add_scalar(\n",
    "                            'eval_{}'.format(key), value, global_step)\n",
    "                    tb_writer.add_scalar('eval_loss', eval_loss, global_step)\n",
    "                    # tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\n",
    "                        'train_loss', (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                # Save model checkpoint\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            epoch_iterator.close()\n",
    "            break\n",
    "\n",
    "    tb_writer.close()\n",
    "    return global_step, tr_loss/global_step, all_eval_results\n",
    "\n",
    "\n",
    "def evaluate(args, eval_dataset, model):\n",
    "    results = {}\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    collate_fn = get_collate_fn(args)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,\n",
    "                                 batch_size=args.eval_batch_size,\n",
    "                                 collate_fn=collate_fn)\n",
    "\n",
    "    # Eval\n",
    "    print(\"***** Running evaluation *****\")\n",
    "    print(\"Num examples: {}; Batch size: {}\".format(len(eval_dataset), args.eval_batch_size))\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for batch in eval_dataloader:\n",
    "    # for batch in tqdm(eval_dataloader, desc='Evaluating'):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = get_input_from_batch(args, batch)\n",
    "\n",
    "            logits = model(**inputs)\n",
    "            tmp_eval_loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = labels.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(\n",
    "                out_label_ids, labels.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    # print(preds)\n",
    "    result = compute_metrics(preds, out_label_ids)\n",
    "    results.update(result)\n",
    "\n",
    "    output_eval_file = os.path.join(args.output_dir, 'eval_results.txt')\n",
    "    with open(output_eval_file, 'a+') as writer:\n",
    "        print('***** Eval results *****')\n",
    "        print(\"eval loss: {}\".format(eval_loss))\n",
    "        for key in sorted(result.keys()):\n",
    "            print(\"{} = {}\".format(key, str(result[key])))\n",
    "    return results, eval_loss\n",
    "\n",
    "\n",
    "def evaluate_badcase(args, eval_dataset, model, word_vocab):\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    collate_fn = get_collate_fn(args)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,\n",
    "                                 batch_size=1,\n",
    "                                 collate_fn=collate_fn)\n",
    "\n",
    "    # Eval\n",
    "    badcases = []\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for batch in eval_dataloader:\n",
    "    # for batch in tqdm(eval_dataloader, desc='Evaluating'):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = get_input_from_batch(args, batch)\n",
    "\n",
    "            logits = model(**inputs)\n",
    "\n",
    "        pred = int(np.argmax(logits.detach().cpu().numpy(), axis=1)[0])\n",
    "        label = int(labels.detach().cpu().numpy()[0])\n",
    "        if pred != label:\n",
    "            sent_ids = inputs['input_ids'][0].detach().cpu().numpy()\n",
    "            aspect_ids = inputs['input_aspect_ids'][0].detach().cpu().numpy()\n",
    "            case = {}\n",
    "            case['sentence'] = args.tokenizer.decode(sent_ids)\n",
    "            case['aspect'] = args.tokenizer.decode(aspect_ids)\n",
    "            case['pred'] = pred\n",
    "            case['label'] = label\n",
    "            badcases.append(case)\n",
    "\n",
    "    return badcases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    return acc_and_f1(preds, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = Parameters()\n",
    "# Required parameters\n",
    "parameters.dataset_name = \"rest\"\n",
    "parameters.output_dir = '/data1/SHENWZH/ABSA_online/data/output-gcn'\n",
    "parameters.num_classes = 3\n",
    "parameters.seed = 2019\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "parameters.glove_dir = \"glove\"\n",
    "parameters.num_layers = 2\n",
    "parameters.add_non_connect = True\n",
    "parameters.multi_hop = True\n",
    "parameters.max_hop = 4\n",
    "parameters.num_heads = 7\n",
    "parameters.dropout = 0.8\n",
    "parameters.num_gcn_layers = 1\n",
    "parameters.gcn_mem_dim = 300\n",
    "parameters.gcn_dropout = 0.2\n",
    "parameters.highway = 'store_true'\n",
    "parameters.bert_model_dir ='bert-base-uncased'\n",
    "parameters.tokenizer = BertTokenizer.from_pretrained(parameters.bert_model_dir)\n",
    "\n",
    "\n",
    "# GAT\n",
    "parameters.gat_attention_type = 'dotprod'\n",
    "parameters.embedding_type = 'bert'\n",
    "parameters.embedding_dim = 300\n",
    "parameters.dep_relation_embed_dim = 300    \n",
    "parameters.hidden_size = 300 \n",
    "parameters.final_hidden_size = 300 \n",
    "parameters.num_mlps = 2\n",
    "\n",
    "# Training parameters\n",
    "parameters.per_gpu_train_batch_size = 16\n",
    "parameters.per_gpu_eval_batch_size = 32\n",
    "parameters.gradient_accumulation_steps = 2\n",
    "parameters.learning_rate = 1e-3\n",
    "parameters.weight_decay = 0.0\n",
    "parameters.adam_epsilon = 1e-8\n",
    "parameters.max_grad_norm = 1.0\n",
    "parameters.num_train_epochs = 30.0\n",
    "parameters.max_steps = -1\n",
    "parameters.logging_steps = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n",
      "{'itos': ['<pad>', '<unk>', 'non-connect', 'ncon_3', 'ncon_2', 'ncon_4', 'det', 'nsubj', 'amod', 'conj', 'pobj', 'dobj', 'cc', 'prep', 'dep', 'poss', 'nn', 'advmod', 'rcmod', 'appos', 'cop', 'nsubjpass', 'num', 'partmod', 'aux', 'ccomp', 'advcl', 'predet', 'auxpass', 'infmod', 'xcomp', 'neg', 'mark', 'possessive', 'pcomp', 'iobj', 'parataxis', 'csubj', 'npadvmod', 'tmod', 'preconj', 'prt', 'expl', 'acomp', 'discourse', 'mwe'], 'stoi': defaultdict(<function _default_unk_index at 0x0000011106F4BD30>, {'<pad>': 0, '<unk>': 1, 'non-connect': 2, 'ncon_3': 3, 'ncon_2': 4, 'ncon_4': 5, 'det': 6, 'nsubj': 7, 'amod': 8, 'conj': 9, 'pobj': 10, 'dobj': 11, 'cc': 12, 'prep': 13, 'dep': 14, 'poss': 15, 'nn': 16, 'advmod': 17, 'rcmod': 18, 'appos': 19, 'cop': 20, 'nsubjpass': 21, 'num': 22, 'partmod': 23, 'aux': 24, 'ccomp': 25, 'advcl': 26, 'predet': 27, 'auxpass': 28, 'infmod': 29, 'xcomp': 30, 'neg': 31, 'mark': 32, 'possessive': 33, 'pcomp': 34, 'iobj': 35, 'parataxis': 36, 'csubj': 37, 'npadvmod': 38, 'tmod': 39, 'preconj': 40, 'prt': 41, 'expl': 42, 'acomp': 43, 'discourse': 44, 'mwe': 45, 'root': 1, 'punct': 1, 'number': 1, 'csubjpass': 1, 'quantmod': 1}), 'len': 46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "D:\\conda\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters 109482240\n",
      "***** Running training *****\n",
      "Num examples: 3602; Epochs: 30.0; Gradient Accumulation steps: 2; Total optimization steps: 3390.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                    | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9199521269117082\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9232469320297241\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   3%|                                                                         | 1/30 [00:20<09:41, 20.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8927600119795118\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9039335301944188\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   7%|                                                                       | 2/30 [00:38<08:57, 19.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9014015402112688\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.903102273600442\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|                                                                    | 3/30 [00:57<08:31, 18.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9150828003883362\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9033745842320579\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9081761598587036\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  13%|                                                                 | 4/30 [01:16<08:20, 19.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9167084063802446\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8939244398048946\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  17%|                                                               | 5/30 [01:35<07:56, 19.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.891795563697815\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8950891273362296\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|                                                            | 6/30 [01:54<07:34, 18.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8947189373629434\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8950902734483991\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  23%|                                                          | 7/30 [02:13<07:14, 18.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.894591497523444\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8990953377314976\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8919509197984423\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  27%|                                                       | 8/30 [02:33<07:01, 19.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9095719626971653\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8919520854949952\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|                                                     | 9/30 [02:51<06:38, 18.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8962955321584429\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8973565484796252\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|                                                  | 10/30 [03:10<06:16, 18.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9357501302446638\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8946474969387055\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  37%|                                               | 11/30 [03:28<05:56, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9247983966554915\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8986073919704982\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8977265468665532\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|                                             | 12/30 [03:48<05:43, 19.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9006741217204502\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9058009556361607\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  43%|                                          | 13/30 [04:07<05:23, 19.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9080647843224662\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9072492974145072\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  47%|                                        | 14/30 [04:26<05:02, 18.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8969031827790397\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9106064302580698\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|                                     | 15/30 [04:44<04:43, 18.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8950497610228402\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9072877049446106\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n",
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.8960868358612061\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  53%|                                   | 16/30 [05:04<04:28, 19.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "Num examples: 1120; Batch size: 32\n",
      "***** Eval results *****\n",
      "eval loss: 0.9092537522315979\n",
      "acc = 0.65\n",
      "f1 = 0.26262626262626265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  53%|                                   | 16/30 [05:14<04:35, 19.64s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-39de7c759a9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mall_eval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_eval_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-0b1763a39130>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args, train_dataset, model, test_dataset)\u001b[0m\n\u001b[0;32m     72\u001b[0m                 model.parameters(), args.max_grad_norm)\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[1;31m# scheduler.step()  # Update learning rate schedule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup CUDA, GPU training\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "parameters.device = device\n",
    "print('Device is {}'.format(parameters.device))\n",
    "\n",
    "# Load datasets and vocabs\n",
    "train_dataset, test_dataset, word_vocab, dep_tag_vocab, pos_tag_vocab= load_datasets_and_vocabs(parameters)\n",
    "\n",
    "print(dep_tag_vocab)\n",
    "\n",
    "# Build Model\n",
    "# model = Aspect_Text_Multi_Syntax_Encoding(args, dep_tag_vocab['len'], pos_tag_vocab['len'])\n",
    "\n",
    "model = Pure_Bert(parameters) # R-GAT + Bert\n",
    "\n",
    "model.to(parameters.device)\n",
    "# Train\n",
    "_, _,  all_eval_results = train(parameters, train_dataset, model, test_dataset)\n",
    "\n",
    "if len(all_eval_results):\n",
    "    best_eval_result = max(all_eval_results, key=lambda x: x['acc']) \n",
    "    for key in sorted(best_eval_result.keys()):\n",
    "        print(\"  %s = %s\", key, str(best_eval_result[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "a04f5d07b0747026a8fbcdf50b9443318e69b1b8bd6247d88bfadb4789282972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
